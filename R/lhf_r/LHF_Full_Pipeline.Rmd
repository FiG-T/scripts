---
title: "LHF Analysis Pipeline"
author: "FiG-T"
data: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
    toc: yes
toc: TRUE
editor_options:
  markdown:
    wrap: 80
---

# Introduction

This markdown document is a guide to the formatting and analysis used on the
"Low Hanging Fruit" (LHF) project. The key aim of this project is to identify
any mitochondrial SNPs whose frequency correlates with climatic or environmental
variables on a global scale. This requires (at the broadest level): acquisition
of the data, alignment of the sequences, filtering of SNP variants, collating
environmental variables, and analysis of any correlations and the testing of
models.

The specific code used to do this is not in it's entirety contained herein.
Instead this document may at some points reference other R scripts which have
further information on a particular aspect of the pipeline.

These scripts (and this markdown) can be found at
[<https://github.com/FiG-T/scripts/tree/main/R/lhf_r>].

## Overview

The following contains a list of steps in order of use:

1.  Download Genbank files from NCBI
2.  Extract relevant information from Genbank files (python)
3.  Convert to FASTA format
4.  Multiple sequence alignment (and filtering)
5.  Converting to VCF format (python)
6.  Filtering VCF files (bash)
7.  Checking allele frequency counts
8.  Collating environmental data
9.  Creating metadata files
10. Visualising genetic variation (inc PCAs)
11. Calculating population allele frequencies & Fst values
12. Creating Trees

## Converting to VCF format

> ***aln .FASTA -\> .vcf***

Here I used anaconda cloud as I was unable to install snp-sites (a python
bioconda package) locally (for unknown and undesipherable reasons).

```{python, include = FALSE, echo = TRUE}

# list available conda environments
conda env list
  # "*" denotes the active environment

# Activate the new environment (or existing one)
`conda activate <environment_name>`

# Install snp-sites 
`conda install -c bioconda snp-sites`

# Upload fasta file to cloud terminal, then: 

# Run snp-sites (changing names as required)
`snp-sites -v -o snp_sites_msa_converted_09_2023.vcf aln_mafft_incRef_2_09_2023.fasta`
```

## Filtering the VCF

The resulting VCF produced above will include any variable site in the
alignment. Many of these will not meet our cutoff criteria (e.g those likely
caused by mapping or sequencing errors) and thus must be removed before any
genetic analysis is attempted. The **easiest way to achieve this is to use
VCFtools**.

VCFtools is a bash program that can be run on the command line (and installed on
MAC iOS via homebrew - `brew install vcftools` ).

For this analysis the reference genome is removed (to avoid duplication effects)
and the minor allele frequency is set to 0.005. This frequency is lower that
standard (0.05) however this filtering is designed to remove spurious mapping
artefacts: a MAF of 0.005 from a data set of 23,807 individuals equates to a
minimum count of 119 - this is unlikely to be reached by chance alone.

```{bash vcftools_filtering, include = FALSE, echo = TRUE}

# set the working directory to the folder with vcf files
cd <path/to/vcfs

# remove the reference individual: 
vcftools --vcf snp_sites_msa_filtered_converted_09_2023.vcf --remove-indv NC_012920.1 --recode --out snp_sites_filtered_vcf_noRef_092023

# create a new VCF with a MAF of 0.005 
vcftools --vcf snp_sites_filtered_vcf_noRef_092023.recode.vcf --maf 0.005 --recode --out snp_sites_filtered_vcf_maf0.005_noRef_092023

# (This was repeated for 0.05 and 0.001 cutoffs)
```

> **This results in a VCF file with 442 sites (of an original 7254) and 23,790
> individuals. This is the file that will be used going forwards.**

### Generating allele frequency information

VCFtools can also be used to calculate allele frequencies and missing data
proportions. These can be used as a useful sanity check for the data.

*Note: the maximum number of alleles here is 3 (not the typical 2) - each of
these will still meet the minimum allele frequency specified above.*

```{bash vcftools_checks, include = FALSE, echo = TRUE}

# calculate the allele frequencies:
vcftools --vcf snp_sites_filtered_vcf_maf0.005_092023.recode.vcf --freq --out ./lhf_allele_freq --max-alleles 3

# calculate the proportion of missing data (where no informative information is available)

# per site: 
vcftools --vcf snp_sites_filtered_vcf_maf0.005_092023.recode.vcf --missing-site --out ./missing_site

# per individual
vcftools --vcf snp_sites_filtered_vcf_maf0.005_092023.recode.vcf --missing-indv --out ./missing_indv
```

## VCF checks & additional processing

> **For additional information on this section, see vcf_processing.R**

To visualise the 'missingness' and allele frequency data generated by vcf tools
[vcftools_checks] we need to load the data into R.

### Allele frequencies

The distribution of the minor allele frequencies is a good check of the quality
of the data, ideally a smooth curve should be observed (highest at very low
frequencies).

```{r check_allele_freq, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)

# load in the data
var_freq <- readr::read_delim(
  file = "~/Documents/data/lhf_d/vcf/lhf_allele_freq_nobase.frq", 
  delim = "\t", 
  col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2", "a3"), 
  skip = 1
)

# calculate the frequency of the minor allele...
var_freq$maf <- var_freq %>% 
  dplyr::select(a1, a2, a3) %>% 
  apply(1, function(z) min(z, na.rm = TRUE))

# plot the distributionn of alleles
ggplot2::ggplot(
  data = var_freq, 
  mapping = ggplot2::aes(maf)
) + 
  ggplot2::geom_histogram(
  bins = 99

) +
  ggplot2::geom_density(
    fill = "orchid3", 
    colour = "black", 
    alpha = 0.75
) +
ggplot2::xlab(
  "Minor Allele Frequency"
) +
ggplot2::ylab(
  "Density"
)
```

This shows some variation from what is typically expected - it is unclear what
exactly is causing this... (I suspect it could be related the the frequencies of
specific mitochondrial haplotypes in specific countries). Looking at the alleles
responsible for the peaks they do not appear upon visual inspection to be
associated with a specific set of countries.

### Missingness

Similar to allele frequencies, it is good to be aware of exactly what data you
may be missing in the data set...

```{r check missingness,  echo=FALSE, warning=FALSE, message=FALSE}

# missing data per site:
var_miss <- readr::read_delim(
  "~/Documents/data/lhf_d/vcf/missing_site.lmiss", 
  delim = "\t",
  col_names = c(
    "chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"
  ), 
  skip = 1)

# plot missingness
ggplot2::ggplot(
  data = var_miss, 
  mapping = ggplot2::aes(fmiss)
) + 
ggplot2::geom_density(
  fill = "orchid3", 
  colour = "black", 
  alpha = 0.3
) + 
ggplot2::xlab(
  "Frequency of missing allele"
) +
ggplot2::ylab(
  "Density"
)

# missing data per individual: 
var_miss <- readr::read_delim(
  "~/Documents/data/lhf_d/vcf/missing_indv.imiss", 
  delim = "\t",
  col_names = c(
    "chr", "pos", "nchr", "nfiltered", "nmiss", "nmiss...5"
  ), 
  skip = 1)

# plot missingness
ggplot2::ggplot(
  data = var_miss, 
  mapping = ggplot2::aes(nmiss...5)
) + 
ggplot2::geom_density(
  fill = "orchid3", 
  colour = "black", 
  alpha = 0.3
) + 
ggplot2::xlab(
  "Frequency of missing data per individual"
) +
ggplot2::ylab(
  "Density"
)
```

Both these plots show that there is no missing data in our VCF file.

### Collecting sample names

During the multiple sequence alignment filtering steps we have removed a number
of individuals (samples) from the original data. For the metadata files (created
later) we need to ensure that the list of samples names with the associated
environmental data *exactly matches* the sample names in the VCF file.

The list of sample names is best queried using **BCFtools**, another command
line tool capable of quickly dealing with VCF (and bcf) files. (It can also be
downloaded using homebrew on Mac OS)

```{bash bcftools, include=FALSE, echo=TRUE}

# navigate to the correct directory:
cd path/to/vcfs

# run bcftools query
bcftools query -l snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf > lhf_filtered_maf0.005_sampleNames.txt

```

Problematic names should have been removed at an earlier stage (but some may
have slipped through the net). To avoid any further issues some name
substitutions can be completed.

```{r sample_names, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

# import bcftools query into R
lhf_names <- readr::read_delim(
  file = "~/Documents/data/lhf_d/vcf/lhf_filtered_maf0.005_sampleNames.txt", 
  delim = "/t", 
  col_names = "samples"
)

# separate out names 
lhf_names <- tidyr::separate(
  data = lhf_names, 
  col = samples, 
  into = c("acc", "a", "b", "c", "d" , "e", "f"), 
  sep = "_"
) 
# combine country words into a single column
lhf_names <- tidyr::unite(
  data = lhf_names, 
  col = "country", 
  a, b, c, d, e , f, 
  sep = " ", 
  na.rm = TRUE
)

# replace names of problem countries
lhf_names$country <- gsub(
  pattern = "USA", 
  replacement = "United States",
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "Great Britain", 
  replacement = "United Kingdom",
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "Czech Republic", 
  replacement = "Czechia", 
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "RussiaRussia", 
  replacement = "Russia", 
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "JapanJapan", 
  replacement = "Japan", 
  x = lhf_names$country
)

# convert to a table column for later:
lhf_names_col <- as.data.frame(lhf_names)

# select unique variables
lhf_names <- unique(lhf_names$country)
lhf_names <- sort(lhf_names)


```

This results in the following list of countries:

```{r country_list, echo=FALSE}
lhf_names
```

## Extracting Climate Variables

Climate data can be downloaded from the
[WorldClim](%5B%5Bhttps://www.worldclim.org/data/bioclim.html%5D%5D(https://www.worldclim.org/data/bioclim.html%5D))
online database; this is derived from remote sensing (satellite) measurements
and covers the years 1970 to 2020 (as of September 2023). A 'historical' dataset
can be obtained which excludes years post 2000 however this is harder to
download (and I was not sure if this was a biologically coherent thing to do).

This section requires extensive use of the `terra` and `geodata` R packages
(which are the successors to `raster` ).

### Downloading the data

```{r terra_install, echo=FALSE}
# if required, download these packages: 
if (!require("geodata")) install.packages("geodata")
if (!require("tidyterra")) install.packages("tidyterra")
```

The data can be downloaded directly using the geodata package:

It is possible to select the resolution of the data (see the WorldClim site for
more details) - this relates to the frequency of the measurements (and thus the
size of each square for each measurement). A resolution of 2.5 minutes (selected
here) roughly equates to each raster square equaling 11 square km.

```{r global_bio, echo=TRUE}
global_bio <- geodata::worldclim_global(
  var = "bio",  # which variable to download
  res = 2.5,  
  path = tempdir(), 
  version = "2.1"
)
# note the output of this is a SpatVector, a S4 class object specific to the terra package.
```

Selecting the **bio** variable downloads a large table of 15 biologically
relevant environmental covariates, for downstream analysis we will un-select
some of the precipitation-based measures. The remaining measures are:

|       |                                            |
|-------|--------------------------------------------|
| BIO1  | Annual mean temperature                    |
| BIO2  | Mean diurnal range                         |
| BIO3  | Isothermality (BIO2 x BIO7)                |
| BIO4  | Temperature Seasonality                    |
| BIO5  | Maximum temperature (of the warmest month) |
| BIO6  | Minimum temperature (of the coldest month) |
| BIO7  | Temperature annual range                   |
| BIO12 | Annual precipitation                       |
| BIO15 | Precipitation Seasonality                  |

```{r select_env_covariates, echo=FALSE}

# select the relevant bio columns
env_covariates <- global_bio[[c(1,2,3,4,5,6,7,12,15)]]

names(env_covariates) <- c(
  "tmp_yr", "tmp_range_drl", "isotherm", "tmp_seasonality", "tmp_max", "tmp_min", 
  "tmp_range_yr", "precip_yr", "precip_seasonality"
)
```

### Generating centroid coordinates

A map of the world with the country boundaries can further be downloaded and
used to generate the centroid coordinates of each country that is present in the
LHF dataset.

```{r world_map, echo=FALSE, message=FALSE}

# download the full map coordinates
world_coord <- geodata::world(
  resolution = 3,  
  # 1 = highest resolution, 5 = lowest resolution
  level = 0, 
  path = tempdir()
)

# generate a dataframe with country names: 
world_coord_df <- as.data.frame(world_coord$NAME_0)

# calculate the centroid coordinates: 
country_centroids <- terra::centroids(
  x = world_coord, 
  inside = TRUE # this will guarantee the point falls within the polygon, but it may not be 
                # the true centroid.
)

# select only country that are relevant to LHF (using the country list generated above)
lhf_centroids <- country_centroids[country_centroids$NAME_0 %in% c(lhf_names),]

# to convert the SpatVector to a dataframe:
# collect country centriod coordinates
world_centroid <- terra::geom(country_centroids)
```

These centroid coordinates can be plotted over a blank map:

```{r centroids_blank_map, echo=FALSE}
plot(world_coord, 
  col = 'snow3', 
  border = "snow4"
)
terra::points(
  lhf_centroids, 
  col = "darkorchid3", 
  pch = 20, 
  cex = 1
)
```

Or over a map with an environmental covariate shown: (Annual temperature range
shown here)

```{r}
bio <- 7
plot(
  env_covariates[[bio]], 
  main = names(env_covariates)[bio]
  )
terra::points(
  lhf_centroids, 
  col = "purple", 
  pch = 20, 
  cex = 1
)
```

To collect ISO information on the full list of country codes:

```{r country_codes, echo=FALSE}
country_codes <- geodata::country_codes(
  query = NULL
)
country_codes$NAME <- gsub(
  pattern = "Czech Republic", 
  replacement = "Czechia", 
  x = country_codes$NAME
)
```

## Generating metadata files

### Extracting data

The first step in creating the metadata files is to extract the relevant data
from the global dataset, there are multiple different ways to do this. It is
hard to determine which of these is best to use, and there are a few pros and
cons for each.

The two methods written out here are:

-   extracting data from country centroid positions

-   extracting mean values for a polygon of each country

#### To extract mean values by region (option 2):

```{r mean_extracted, echo=TRUE}

lhf_env <-   terra::extract(
  env_covariates, 
  world_coord, 
  mean,
  na.rm = TRUE, 
  weights = TRUE
)
```

To combine this with the country names, coordinates, and sample IDs:

```{r meta2_processing, echo=TRUE}

# combine with all country names:
lhf_env. <- base::cbind(
  world_coord_df, 
  lhf_env
)

# to add country centroids:
lhf_env. <- base::cbind(
  lhf_env., 
  world_centroid
)

# reorder columns (and remove unwanted):
lhf_env. <- lhf_env.[,c(1,14,15,3:11)]

# rename wordy column names:
names(lhf_env.)[1:3] <- c("country", "long", "lat")

lhf_env.$country <- gsub(
  pattern = "Czech Republic", 
  replacement = "Czechia", 
  x = lhf_env.$country
)
```

```{r meta2_file_creation, echo=FALSE}

# join the lhf_env table to the list of names 
  # as left_join is used only countries matching 
lhf_meta2 <- dplyr::left_join(
  x = lhf_names_col, 
  y = lhf_env.,
  by = dplyr::join_by(country)
)

# add in regions and continents: 
lhf_meta2 <- dplyr::left_join(
  x = lhf_meta2, 
  y = country_codes, # quereied above
  by = dplyr::join_by("country" == "NAME")
)

# reorder and remove unwanted cols: 
lhf_meta2 <- lhf_meta2[,c(1:4,19, 14, 15, 20, 22, 5:13)]
```

```{r meta2_format, echo=FALSE}
# unite first two columns to reform sample names: 
lhf_meta2 <- tidyr::unite(
  data = lhf_meta2, 
  col = "samples", 
  acc, country,
  sep = "_", 
  na.rm = TRUE, 
  remove = FALSE
)

# remove spaces from sample names: 
lhf_meta2$samples <- gsub(
  pattern = " ", 
  replacement = "_", 
  x = lhf_meta2$samples
)

# change iso column names (to remove capital letters)
names(lhf_meta2)[6:9] <- c("sovereign", "iso3","iso2", "subcontinent")

```

Save this meta file (so that you do not need to rerun this for every script).
Feather format was chosen here as it allows for extremely rapid download and
upload into R and python.

```{r meta2_feather, echo=TRUE}

# write to alternate feather file 
feather::write_feather(
  x = lhf_meta2, 
  path = "~/Documents/data/lhf_d/lhf_meta2_maf0.005_data_11_2023.feather"
  # change this path to path the desired destination
)

# save file grouped by country: 
feather::write_feather(
  x = lhf_env., 
  path = "~/Documents/data/lhf_d/lhf_country_metadata_maf0.005_11_2023.feather"
  # change this path to path the desired destination
)

```

## Visualising Genetic Variation

For further information on this script, see **vcf_freq_env_visualisation.R**.

### Setup

This section utilises the *adegenet*, parallel and *mappplot* packages. These
may need to be downloaded:

```{r adegent_install, echo=FALSE}
if (!require("adegenet")) install.packages("adegenet")
if (!require("parallel")) install.packages("parallel")
if (!require("mapplots")) install.packages("mapplots")

# the following libraries are also required for this section: 
library(vcfR)
library(feather)
library(adegenet)
library(parallel)
library(ggplot2)
library(tidyr)
library(dplyr)
library(maps)
library(mapplots)
```

#### Data required:

Two files are required for this section: the filtered VCF and the associated
meta data file (see above for how to make these).

> The row names in the meta file must *exactly match* the sample names in the
> VCF.

```{r upload_paired_files, echo=TRUE}

# read in vcf file 
lhf_vcf <- vcfR::read.vcfR(
  file = "~/Documents/data/lhf_d/vcf/snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf", 
  # this file is the same as the one used for vcf processing
  verbose = TRUE
)

# note the second meta data file is used here (with country-wide averages)
lhf_meta2 <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_meta2_0.005_data_11_2023.feather"
)

```

#### Converting to a Genlight object

The adegenet (and other packages) require the vcf data to be in a "Genlight"
format (this is another S4 data class). To convert the vcfR to a genlight
object:

```{r create_genlight, echo=TRUE}

# this can be done using the vcfR package:
vcf.gl <- vcfR::vcfR2genlight(
  x = lhf_vcf
) # this will omit any non-biallelic sites
```

adegenet::pop(vcf.gl)

```{r genlight_meta, echo=TRUE}

#  Specify the country IDs
adegenet::pop(vcf.gl) <- lhf_meta2$country

adegenet::ploidy(vcf.gl) <- 1 # is this a correct assumption?

# view genlight object 
vcf.gl

```

In this example *country* information is specified to the 'population'
variable - this could also be continent or subcontinent information (or any
other defining variable).

We can use this genlight object to run Principal Component Analysis and
calculate the allele frequencies.

### PCA plots

To generate the pca scores and eigenvalues we can use the *adegenet* package...

***Note: This takes a very large (many hours) amount of time to run (given the
very large number of sequences).***

```{r create_pca, echo=FALSE}

vcf.pca <- adegenet::glPca(
  x = vcf.gl, 
  nf = 4,          # how many PCs to retain
  parallel = TRUE, # unless specified otherwise, the max number of available cores will be used. 
)

```

Once generated, we can convert the PCA scores into a tibble (and save this).

```{r format_pca_scores, echo=FALSE}

# convert to a tibble
vcf.pca.scores <- tidyr::as_tibble(vcf.pca$scores)

# as the pca takes a long time to run ... save these scores... 
feather::write_feather(
  x = vcf.pca.scores, 
  path = "~/Documents/data/lhf_d/lhf_pca_scores_0.005.feather"
)

```

We can add information from the metadata file to the PCA scores:

```{r pca_meta2, echo=TRUE}

# add in population data (country used as an example here)
vcf.pca.scores$country <- lhf_meta2$country
vcf.pca.scores$subcontinent <- lhf_meta2$subcontinent
vcf.pca.scores$continent <- lhf_meta2$continent


# add in env data (edit to chose, this could be any column in the metadata file)
vcf.pca.scores$lat <- lhf_meta2$lat

```

As a sanity check/ precaution, we can plot the PCs vs the proportion of the
variance that they explain, as well as calculating the exact proportion for a
specific component:

```{r pca_contributions, echo=TRUE}

# to create the overall plot:
graphics::barplot(
  100 * vcf.pca$eig / sum(vcf.pca$eig),
  space = 0.1,
  colour = "darkseagreen"
)

# calculate the sum of all eigenvalues
 eig.total <- sum(vcf.pca$eig)

# check the % for each of the primary PCs
for (i in 1:4){
  print(i)
  print(
    formatC(
      head(vcf.pca$eig)[i]/eig.total * 100
    )
  )
 # where i is the number of the PC you wish to show...
}
```

#### Creating the plots:

The vcf scores above can be plotted to show the distribution of samples, these
an be coloured/shaped by any of the variables in the meta file which have been
added to the PCA scores tibble (see above: chunk pca_meta2).

Given the very large number of countries in the dataset, colouring by this value
is not advised (continents gives a much clearer overview).

```{r pca_plots, echo=TRUE}

# plotting PC1 vs PC2
 ggplot2::ggplot(
   data = vcf.pca.scores, 
   mapping = ggplot2::aes(
     x = PC1, 
     y = PC2, 
     col = continent
   )
) +
ggplot2::geom_point(
)

# plotting PC3 vs PC4
 ggplot2::ggplot(
   data = vcf.pca.scores, 
   mapping = ggplot2::aes(
     x = PC3, 
     y = PC4, 
     col = continent
   )
) +
ggplot2::geom_point(
)
```

#### Identify the SNPs with greatest influence:

We can use PCA weighting as a proxy for which SNPs have the highest influence
over genetic variation.

```{r pca_weightings, echo=FALSE}

# select PCA loading values for the 4 Principal Components
snp_loadings <- tidyr::as_tibble(
  data.frame(
  vcf.gl@loc.names, 
  vcf.pca$loadings[,1:4]
  )
)

# rank the SNPs by their weighting for PC1 (Axis 1)
snp_loadings <- dplyr::arrange(
  .data = snp_loadings, 
  desc(Axis1)
)
```

```{r snp_loadings, echo=FALSE}
head(snp_loadings)
```

## Calculating Population Allele Frequencies

Calculating the allele frequencies by population is valuable for both
visualising the data and is necessary for calculating population Fst scores.

```{r pop_allele_freq, echo=TRUE}

pop <- as.factor(c(lhf_meta2$country))

pop_diffs <- vcfR::genetic_diff(
  vcf = lhf_vcf, 
  pops = pop
  # uses Nei's distance as standard
)
```

This data will require reformatting:

```{r pop_AF_formatting, echo=FALSE}

allele_freq <- pop_diffs[,c(1:149)] # change ncol to fit the data (i.e: no. countries)

allele_freq <- reshape2::melt(
  allele_freq
)

names(allele_freq)[3:4] <- c("location", "a_freq") 

# remove erronous prefixes:
allele_freq$location<- gsub(
  pattern = "Hs_", 
  replacement = "", 
  x = allele_freq$location
)
allele_freq$location <- gsub(
  pattern = "n_", 
  replacement = "", 
  x = allele_freq$location
)

```

This data can be combined with mean coordinates (used for plotting). If
populations have been assigned at the country level 'lhf_centroids' can be used
as the coordinates for this. If subcontinent or continent is assigned to the
population data, mean coordinates have to be generated for these.

```{r continent_coord, echo=FALSE, message=FALSE}

# select continent/country/subcontinent as required
location_coords <- data.frame(
  lhf_meta2$country, lhf_meta2$lat, lhf_meta2$long, lhf_meta2$iso3
)

# requires dplyr to be loaded...
library(dplyr)
location_coords <- location_coords %>%
  group_by(lhf_meta2.country) %>%
  mutate(lat = mean(lhf_meta2.lat)) %>%
  mutate(long = mean(lhf_meta2.long))

location_coords <- location_coords[,-c(2,3)]
location_coords <- unique(location_coords)
names(location_coords)[c(1,2)] <- c("location", "iso3")

```

Once coordinates are available, these can be joined to the allele frequency
data:

```{r AF_join_coord, echo=FALSE}

# if using continents: 
allele_freq_location <- dplyr::left_join(
  x = allele_freq, 
  y = location_coords, 
  by = join_by("location" == "location")
)

# calculate which countries have very low N and remove:
country_N <- lhf_meta2 %>%
  group_by(country) %>%
  summarise(
    N = n()
  )

country_N <- country_N %>%
  filter(N >= 20) 
country_N <- c(country_N$country)

allele_freq_location <- allele_freq_location[allele_freq_location$location %in% c(country_N),]

head(allele_freq_location)
```

```{r AF_join_coord_country}
# NOTE: This will not run unless country_codes have been queried above, this needs to be added in...
#
# if using countries: 

allele_freq_location <- dplyr::left_join(
  x = allele_freq_location, 
  y = country_codes,   
  by = join_by("location" == "NAME")
)

allele_freq_location <- allele_freq_location[,c(1:8, 13, 15)]
head(allele_freq_location)

```

Save this file:

```{r AF_download}

feather::write_feather(
  x = allele_freq_location, 
  path = "~/Documents/data/lhf_d/lhf_country_allele_freq_maf0.005.feather"
)

```

### Plotting Allele Frequencies

Frequencies of specific alleles can also be plotted on a world map.

E.g.: For allele 9607 (which has the largest loadings on the PCA generated
above)

```{r select_allele, echo=TRUE}

# isolate the allele of interest: 
allele_freq_select <- allele_freq_location[allele_freq_location$POS == "9607",]

```

```{r plot_af, echo=FALSE, message=FALSE, warning=FALSE}

# using coordinates gathered earlier: 
plot(
  world_coord, 
  col = 'snow3', 
  border = "snow3"
)

for (i in 1:nrow(allele_freq_select)) {
  mapplots::add.pie(
    z = c(
      allele_freq_select$a_freq[i], 
      1- allele_freq_select$a_freq[i]
    ), 
    x = allele_freq_select$long[i], 
    y = allele_freq_select$lat[i], 
    radius = 3, 
    col = c(
      ggplot2::alpha("cadetblue3", 0.8), ggplot2::alpha("orchid2", 0.8)
    ), 
    labels = allele_freq_select$iso3[i], 
    #label.dist = 1.1, 
    cex = 0.5
  )
}

```

To plot by continent:

```{r a_freq_continent, include=FALSE}

pop <- as.factor(c(lhf_meta2$subcontinent))

continent_diffs <- vcfR::genetic_diff(
  vcf = lhf_vcf, 
  pops = pop
  # uses Nei's distance as standard
)

continent_allele_freq <- continent_diffs[,c(1:ncol(continent_diffs))] # change ncol to fit the data (i.e: no. countries)

continent_allele_freq <- reshape2::melt(
  continent_allele_freq
)

names(continent_allele_freq)[3:4] <- c("location", "a_freq") 

# remove erronous prefixes:
continent_allele_freq$location<- gsub(
  pattern = "Hs_", 
  replacement = "", 
  x = continent_allele_freq$location
)
continent_allele_freq$location <- gsub(
  pattern = "n_", 
  replacement = "", 
  x = continent_allele_freq$location
)

# select continent/country/subcontinent as required
continent_coords <- data.frame(
  lhf_meta2$subcontinent, lhf_meta2$lat, lhf_meta2$long
)

# requires dplyr to be loaded...
continent_coords <- continent_coords %>%
  group_by(lhf_meta2.subcontinent) %>%
  mutate(lat = mean(lhf_meta2.lat)) %>%
  mutate(long = mean(lhf_meta2.long))

head(continent_coords)

continent_coords <- continent_coords[,-c(2,3)]
continent_coords <- unique(continent_coords)
names(continent_coords)[1] <- "location"

# if using continents: 
allele_freq_continent <- dplyr::left_join(
  x = continent_allele_freq, 
  y = continent_coords, 
  by = join_by("location" == "location")
)
```

```{r plot_AF_continent, echo=FALSE}
#allele_freq_continent <- allele_freq_continent[,c(1:8, 13, 15)]

allele_freq_select <- allele_freq_continent[allele_freq_continent$POS == "9607",]

# using coordinates gathered earlier: 
plot(
  world_coord, 
  col = 'snow3', 
  border = "snow3"
)

for (i in 1:nrow(allele_freq_select)) {
  mapplots::add.pie(
    z = c(
      allele_freq_select$a_freq[i], 
      1- allele_freq_select$a_freq[i]
      ), 
    x = allele_freq_select$long[i], 
    y = allele_freq_select$lat[i], 
    radius = 7, 
    col = c(
      ggplot2::alpha("cadetblue3", 0.8), ggplot2::alpha("orchid2", 0.8)
    ), 
    labels = allele_freq_select$location[i], 
    label.dist = 1.1, 
    cex = 0.5
  )
}
```

## Generating Fst Scores

For downstream analyses it is necessary to have measures of genetic diversity
between populations. This is first used to construct a tree, which is
subsequently used in the PGLS analysis.

```{r}
# load in required libraries
if (!require("hierfstat")) install.packages("hierfstat")

library(hierfstat)
library(tidyr)

```

#### Reading in the data

If the above script has been run previously (and you wish to continue onwards
from here), the feather files generated earlier can be used to read the data
back into R.

```{r importing_feather_AF_location, echo=FALSE}

# this imports in the file from above (hence this step may not be needed in the table is already present in the environment)

# read in vcf file 
lhf_vcf <- vcfR::read.vcfR(
  file = "~/Documents/data/lhf_d/vcf/snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf", 
  # this file is the same as the one used for vcf processing
  verbose = TRUE
)

```

This data can be formatted to only include information on the position, country
and allele frequencies. It is then converted to a "genid" object (another type
of S4 class object)

```{r create_genid_pop, echo=FALSE}

lhf_genid <- vcfR::vcfR2genind(
  lhf_vcf
)

# add in country data
adegenet::pop(lhf_genid) <- lhf_meta2$country

head(lhf_genid$pop)

```

```{r calculate_fst, echo=FALSE}

lhf_fstat <- hierfstat::genind2hierfstat(lhf_genid)

lhf_fst <- hierfstat::pairwise.neifst(
  dat = lhf_fstat, 
  diploid = FALSE
)

head(lhf_fst)
class(lhf_fst)
```

As this takes a little while to run it is worth saving the output in a separate
file:

```{r save_pairwise_fst,, echo=FALSE}
# save the data (as this takes a while to run)
feather::write_feather(
  x = as.data.frame(lhf_fst), 
  path = "~/Documents/data/lhf_d/lhf_pairwise_fst_maf0.005_112023.feather"
)
```

### Creating trees

The computed Fst scores can be used directly to create a phylogentic tree, or
the can be used to calculate the genetic distance between populations (which is
then used for tree building purposes).

```{r}
country_N <- lhf_meta2 %>%
  group_by(country) %>%
  summarise(
    N = n()
  )

country_N <- country_N %>%
  filter(N >= 20) 
country_N <- c(country_N$country)

lhf_fst_filtered <- lhf_fst[(rownames(lhf_fst) %in% country_N),
                            (colnames(lhf_fst) %in% country_N) ]
```

```{r building_trees, echo=FALSE}
# generate a tree from the fst values
lhf_tree <- ape::njs(lhf_fst_filtered)

# create a new tibble for the tree metadata 
tree_meta <- dplyr::as_tibble(lhf_tree$tip.label)
names(tree_meta) <- "country"

# combine populations (countries) with the associated metadata
tree_meta <- dplyr::left_join(
  tree_meta, 
  country_codes,  # this is queried above
  by = join_by("country"=="NAME")
)

# select only country, iso3, subcontinent and continent 
tree_meta <- tree_meta[,c(1,2,8,10)]
names(tree_meta)[c(2,3)] <- c("iso3", "subcontinent")
```

For aesthetic purposes, colors can be assigned to different continents

```{r set_tree_colours, echo=FALSE}

# create a new tibble with continent list
tree_colour <- dplyr::as_tibble(
  unique(
    tree_meta$continent
  )
)

# assign a colour to each continent
tree_colour$cont_colour <- c(
  "darkorchid3", "firebrick3", "chartreuse3", 
  "chartreuse4", "turquoise", "deepskyblue4" # "lightgrey" - remove if no NAs
)

# combine this with the tree metadata table 
tree_meta <- dplyr::left_join(
  x = tree_meta, 
  y = tree_colour, 
  by = join_by("continent"=="value")
)

# create a vector with the associated colour for each country (split by continent)
tree_colours <- tree_meta$cont_colour


```

As no outgroup is available, we midpoint root this tree. A key assumption of
this is that all branches are evolving at the same rate (- is this true here??)

```{r midpoint_root}

ape::plot.phylo(
  x = lhf_tree, 
  type = "fan",
  use.edge.length = FALSE, 
  cex = 0.7, 
  #srt = 20, 
  label.offset = 0.05, 
  tip.color = tree_colours
)

rooted_tree <- phangorn::midpoint(lhf_tree)

ape::plot.phylo(
  x = rooted_tree, 
  type = "fan",
  use.edge.length = FALSE, 
  cex = 0.7, 
  #srt = 20, 
  label.offset = 0.05, 
  tip.color = tree_colours
)

```

Saving the tree:

```{r}
ape::write.tree(
  rooted_tree, 
  file = "~/Documents/data/lhf_d/lhf_pairwise_tree_rooted_midpoint.newick"
)
```

#### Filtering the tree

To ensure that the tree is as accurate as possible, countries with a small
number of individuals will be removed from the analysis:

```{r fig.height=6, fig.width=6}
# looking at problem countries on the tree... 
# removing Canada, Finland, Netherlands, and Belarus (for now)
problem_countries <- c("Canada", "Belarus", "Finland", "Netherlands")

rooted_tree_filtered <- ape::drop.tip(
  phy = rooted_tree, 
  tip = problem_countries, 
  trim.internal = TRUE
)

# create a new tibble for the tree metadata 
tree_meta <- dplyr::as_tibble(rooted_tree_filtered$tip.label)
names(tree_meta) <- "country"

# combine populations (countries) with the associated metadata
tree_meta <- dplyr::left_join(
  tree_meta, 
  country_codes,  # this is queried above
  by = join_by("country"=="NAME")
)

# select only country, iso3, subcontinent and continent 
tree_meta <- tree_meta[,c(1,2,8,10)]
names(tree_meta)[c(2,3)] <- c("iso3", "subcontinent")

tree_colour <- dplyr::as_tibble(
  unique(
    tree_meta$continent
  )
)

# assign a colour to each continent
tree_colour$cont_colour <- c(
  "darkorchid3", "firebrick3", "chartreuse3", 
  "chartreuse4", "turquoise", "deepskyblue4" # "lightgrey" - remove if no NAs
)

# combine this with the tree metadata table 
tree_meta <- dplyr::left_join(
  x = tree_meta, 
  y = tree_colour, 
  by = join_by("continent"=="value")
)

# create a vector with the associated colour for each country (split by continent)
tree_colours <- tree_meta$cont_colour

ape::plot.phylo(
  x = rooted_tree_filtered, 
  type = "fan",
  use.edge.length = FALSE, 
  cex = 0.7, 
  #srt = 20, 
  label.offset = 0.05, 
  tip.color = tree_colours
)

```

```{r}
# save this tree 
ape::write.tree(
  rooted_tree_filtered, 
  file = "~/Documents/data/lhf_d/lhf_pairwise_tree_rooted_midpoint_filtered.newick"
)
```

We can test the quality of this tree by perfoming a bootstrapping analysis ---
how does this work for a distance matrix created from population Fst values?
Surely it is the same every time?

## PGLS Analysis & Descriptive Correlations

This section focuses on on how to perform the PGLS analysis (between different
populations) -

```{r pgls libraries needed, echo=FALSE, message=FALSE, warning=FALSE}
if (!require("psych")) install.packages("psych")
if (!require("psychTools")) install.packages("psychTools")
if (!require("caper")) install.packages("caper")

library(psych)
library(psychTools)
library(tidyr)
library(caper)
library(dplyr)
library(MuMIn)
library(stringr)
library(ggplot2)
library(Hmisc)

```

Reading in the data:

This section requires information on the allele frequencies split over
populations and the tree (both generated above).

```{r pgls_required_data}

allele_freq_location <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_country_allele_freq_maf0.005.feather"
)

lhf_env. <- feather::read_feather(
   path = "~/Documents/data/lhf_d/lhf_country_metadata_maf0.005_11_2023.feather"
)

# combine the environmental data with the population allele frequencies
allele_freq_location <- dplyr::left_join(
  x = allele_freq_location, 
  y = lhf_env.[,-c(2,3)], 
  by = join_by("location"=="country")
)

# read in the tree 
rooted_tree_filtered <- ape::read.tree(
  file = "~/Documents/data/lhf_d/lhf_pairwise_tree_rooted_midpoint_filtered.newick"
)

```

#### Selecting a test set of SNPs:

```{r select_test_snps}
# list all unique snps
#unique(allele_freq_location$POS)

# randomly sample 15 locations
test_snps <- sample(
  unique(allele_freq_location$POS), 
  size = 15
)

# show these locations
print(
  test_snps
) 

# Note: in order to be ordered this values have to be converted into numerics - this is not compatibility with downstream steps. 

```

```{r pivot_test_AF}

# filter to only include test snps
lhf_test_AF <- allele_freq_location %>%
  dplyr::filter(
    POS %in% test_snps
  )
unique(lhf_test_AF$POS)

# convert to wide format
lhf_test_AF <- tidyr::pivot_wider(
  data = lhf_test_AF, 
  names_from = POS, 
  values_from = a_freq, 
  names_prefix = "pos_"
)

head(lhf_test_AF)

```

#### Formatting the full set of SNPs

Although the test set of SNPs will be used for the following exploratory
analysis, the full panel will be necessary for later downstream analysis.

```{r pivot_AF}

## Note: this requires the pgls_required_data chunk to have been run. 

lhf_AF <- tidyr::pivot_wider(
  data = allele_freq_location, 
  names_from = POS, 
  values_from = a_freq, 
  names_prefix = "pos_"
)

lhf_AF.df <- as.data.frame(
  lhf_AF
)

cdat <- caper::comparative.data(
  data = lhf_AF.df, 
  phy = rooted_tree_filtered,  #imported in above (if not already present)
  names.col = "location", 
  na.omit = TRUE
)

```

#### Comparing environmental variables

```{r plot_env_corr}

# plot correlations: 
#  between environmental variables:
psych::pairs.panels(
  x = lhf_test_AF[,c(4:14)], 
  lm = TRUE, 
  stars = TRUE, 
  hist.col = "orchid3"
)

```

```{r}
# between some snps (and environmental variables)
psych::pairs.panels(
  x = lhf_test_AF[,c(15:29)], 
  lm = TRUE, 
  stars = TRUE, 
  hist.col = "orchid3"
)
psych::pairs.panels(
  x = lhf_test_AF[,c(sample(4:14, 2), sample(15:29, 10))], 
  lm = TRUE, 
  stars = TRUE, 
  hist.col = "orchid3"
)
```

```{r creating_test_cdat}
# convert the test AF from a tibble to a dataframe (so that the caper package works)
lhf_test_AF.df <- as.data.frame(
  lhf_test_AF
  )

cdat <- caper::comparative.data(
  data = lhf_test_AF.df, 
  phy = rooted_tree_filtered,  #imported in above (if not already present)
  names.col = "location", 
  na.omit = TRUE
)

```

### Model selection

Broadly speaking, there are two ways of testing models; create a full model then
test whether you can remove a variable without impacting the model OR start with
a simplistic model and build up.

#### Issues with Co-linearity

As many of the environmental variables are highly correlated (see
[plot_env_corr]) there are significant problems with creating complex models
(with more than 5 predictor variables).

A number of potential solutions were discussed in response to this:

-   **First generating a 'simplistic' model with latitude only**, *with
    significant SNPs followed up with downstream analysis focusing on specific
    environmental traits*. The assumption here is that as latitude correlates
    well with many variables and will therefore pick up any highly significant
    hits. An additional advantage is that this can be run for all SNPs, removing
    the requirement for SNP-specific model selection.

-   Selection of key "biologically relevant variables". This is similar to
    above, where instead of latitude only, specific variables of perceived
    interest are selected, with these simpler models run for all sites. This is
    advantageous as it includes more information on specific traits (making
    inferences easier), but is disadvantaged by the fact it may not result in
    the most efficient model being fitted for each site.

-   Sequential addition of uncorrelated environmental variables. (Using the
    following method...)

> **Method**
>
> Starting with latitude I sequentially look at the next environmental variable
> (in order of perceived biological relevance). If the correlation is above 0.75
> with any of the existing variables in the model, the variable is skipped. If
> the variable has no correlations stronger that 0.75 with any other variables,
> the variable is added and the process is repeated.
>
> Once all variables have been processed, this "nearly full" model is used for a
> top-down approach of model selection.

> 1.  Starting with latitude.
> 2.  tmp_max added
> 3.  tmp_min added
> 4.  isothermality skipped
> 5.  tmp_seasonality skipped
> 6.  tmp_yr skipped
> 7.  tmp_range_yr skipped
> 8.  precip_yr added
> 9.  precip_seasonality added
> 10. long added

The third method described here can scripted by:

```{r pgls_model selection}

# starting with pos_7909 ~lat 
# 
pgls_mod <- caper::pgls(
  formula = pos_7909 ~ lat , 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7909 ~ lat + tmp_min, 
  data = cdat, 
  #lambda = "ML"
)

print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7909 ~ lat + tmp_min + precip_yr, 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7909 ~ lat + tmp_min + precip_yr + precip_seasonality , 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7909 ~ lat + tmp_min + precip_yr + precip_seasonality + tmp_max, 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7909 ~ lat + tmp_min + precip_yr + precip_seasonality + tmp_max , 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)
#anova_df <- anova(pgls_mod)
#anova_df
```

> The conclusion from this method is that model 2 (with latitude and minimum
> temperature) is the best fit for this specific SNP.

**While this final method has it's merits, option 1 (fitting all SNPs with
latitude only as a predictor) was chosen going forward given the issues of
running model selection for each of the SNPs separately.**

**Note: Although multiple SNPs are highly correlated, this is not an issue here
as in each model only a single SNP is given as the response variable (hence any
other SNPs become irrelevant).**

```{r model_testing, include=FALSE}
# change na options
options(na.action = "na.fail")

MuMIn::dredge(
  global.model = pgls_mod, 
  beta = "none", 
  evaluate = TRUE, 
  rank = "AICc"
)
# isothem, lat, max, min 
options(na.action = "na.omit")

```

### Assessing SNP linkage

Given many of the SNPs are in close proximity to each other, and there are very
low levels of recombination in the mitochondrial genome, there is likely a high
degree of correlation (i.e: linkage) between some sets of allele frequencies. In
an attempt to identify areas where there are exceptionally high levels of
correlation the following heat maps can be created. (In effect this are LD
plots)

```{r AF_heatmaps, fig.height=5, fig.width=10}

col_N <- ncol(lhf_AF) # calculate the total number of columns
n_split <- 3 # select the number of heatmaps you want 
             # e.g.: an n_split of 2 will plot the first half then the second half.

for (i in 1:n_split) {
  
  if (i == 1){
    x <- 15 
  } else {
    x <- x.
  }
  
  if (i ==1) {
    y <-  i*((col_N/n_split))
    y <- base::round(y)
  } else {
    y <- i*(col_N/n_split)
    y <- base::round(y)
  }
  
  print(x)
  print(y)
  
  AF_cor <- Hmisc::rcorr(
    as.matrix(
    lhf_AF[,x:y]
    )
  )
  
  x. <- y 
  
  AF_cor$r[lower.tri(AF_cor$r)] <- NA

  AF_cor <- reshape2::melt(
    AF_cor$r,
    na.rm = TRUE
  )
  
p <- ggplot2::ggplot(
    data = AF_cor,
    ggplot2::aes(
      x = Var1,
      y = Var2,
      fill = value
      )
    ) +
    geom_tile(
      colour = "white"
    ) +
    scale_fill_gradient2(
      low = "deepskyblue4",
      high = "darkorchid3",
      mid = "snow3",
      midpoint = 0,
      #limit = c(0.5,1),
    ) +
 #  ggtitle(
 #     paste("AF_heatmap", i)
 #   ) +
   theme(
     axis.text.x = element_text(
       angle = 55,
       vjust = 0.5,
       hjust = 0.5,
       size = 5
       ), 
     axis.text.y = element_text(
       angle = 5,
       vjust = 0.5,
       hjust = 0.5, 
       size = 4
       ), 
     axis.title = element_blank(), 
     legend.position = "none"
   )
  
  print(p)
}
```

To calculate the matrix for a specific set of columns:

```{r AF_cor_specific}

AF_cor <- Hmisc::rcorr(
    as.matrix(
    lhf_AF[,4:460]
    )
)

# AF_cor$r[lower.tri(AF_cor$r)] <- NA

# AF_cor$r <- AF_cor$r[12:nrow((AF_cor$r)), 1:11]
AF_cor$r <- AF_cor$r[ 12:200, 1:11]

View(AF_cor$r)

AF_cor <- reshape2::melt(
  AF_cor$r,
  na.rm = TRUE
  )

message("AF_cor created")

ggplot2::ggplot(
    data = AF_cor,
    ggplot2::aes(
      x = Var2,
      y = Var1,
      fill = value
      )
    ) +
    geom_tile(
      colour = "white"
    ) +
    scale_fill_gradient2(
      low = "deepskyblue4",
      high = "darkorchid3",
      mid = "snow3",
      midpoint = 0,
      #limit = c(0.5,1),
      #space = "Lab",
      #name="Pearson\nCorrelation"
    ) +
   theme(
     axis.text.x = element_text(
       angle = 55,
       vjust = 0.5,
       hjust = 0.5,
       size = 5
       ), 
     axis.text.y = element_text(
       angle = 5,
       vjust = 0.5,
       hjust = 0.5, 
       size = 4
       ), 
    # axis.title = element_blank(), 
    # legend.position = "none"
   )
    

```

### Generating ANOVA tables

Once the model is selected, ANOVA tables can be generated for each SNP.

```{r generating_ANOVA_tables}

for (i in 18:ncol(lhf_AF.df)){
  
  # select position
  snp <- names(lhf_AF.df)[i]
  
  snp_pos <- stringr::str_extract(
    string = snp, 
    pattern = "\\d+"
  )
  
  # create formula
  formula <- as.formula(
    paste(
      snp, " ~ lat "
    )
  )
  
  # run model
  mod <- caper::pgls(
    formula = formula, 
    data = cdat, 
    #lambda = "ML"
  )
  
  # create anova table:
  anova_df <- anova(mod)
  
  anova_df <- cbind(anova_df, snp_pos)
  
  anova_df <- anova_df %>%
    mutate(covariate = rownames(anova_df))
  
  if (i == 18){
    anova_scores <- anova_df
  } else {
    anova_scores <- rbind(
      anova_scores, 
      anova_df
    )
  }
  
  if (i == ncol(lhf_AF.df)){
    return(anova_scores)
  }
  
}
```

```{r bonferroni_corrections}
bon_threshold <- 0.05/(ncol(lhf_AF.df)-8)*1
```

```{r manhattan_plots, echo=FALSE}

ggplot2::ggplot(
  data = anova_scores, 
  mapping = ggplot2::aes(
    x = as.numeric(snp_pos), 
    y = log10(`Pr(>F)`), 
    colour = covariate 
    )
  ) + 
  ggplot2::geom_point(
  ) +
  ggplot2::scale_colour_manual(
  #  values = c("chartreuse4","darkorange3", "deepskyblue2",
  #             "lightgrey", "darkorchid3","orchid1", "black")
    values = c("deepskyblue4", "grey")
  ) +
  ggplot2::geom_line(
    y = -1*log10(0.05), 
    colour = "black", 
    size = 1
  ) +
  ggplot2::xlab(
    "SNP position"
  ) + 
  ggplot2::ylab(
    "Logged P-value"
  ) + 
  ggplot2::scale_x_continuous(
    breaks = seq(0,17000, 1000)
  ) +
  ggplot2::ylim(c(0, -100)
  ) +
  ggplot2::ylab(
    "Logged P-value"
  ) +
  ggplot2::theme(
    legend.position = "bottom", 
    axis.line = element_line(
      linewidth = 1, 
      colour = "black"
    ), 
    plot.background = element_rect(
      fill = "snow3"
    ), 
    panel.grid = element_line(
      colour = "snow3", 
      linetype = "dotdash"
    ),
    panel.grid.major.x = element_line(
      colour = "snow4", 
      linetype = "dashed"
    ), 
    legend.background = element_rect(
      fill = "snow3"
    )
  )


```

```{r}
ggplot2::ggplot(
  data = anova_scores, 
  mapping = ggplot2::aes(
    x = as.numeric(snp_pos), 
    y = log10(`Pr(>F)`), 
    colour = covariate 
    )
  ) + 
  ggplot2::geom_point(
  ) +
  ggplot2::scale_colour_manual(
  #  values = c("chartreuse4","darkorange3", "deepskyblue2",
  #             "lightgrey", "darkorchid3","orchid1", "black")
    values = c( "chartreuse3", "chartreuse4", "darkorchid", "darkorchid4","darkorange3" ,"orchid", "deepskyblue4", "lightgrey")
  ) +
  ggplot2::geom_line(
    y = -1*log10(0.05), 
    colour = "black", 
    size = 1
  ) +
  ggplot2::geom_segment(
    data = mt_gene_pos,
    ggplot2::aes(
      x = Starting, 
      xend = Ending, 
      y = -80, 
      yend = -80, 
      col = complex
    ),
    size = 6, 
    alpha = 0.4, 
    position = position_jitter(
      height = 3
    )
  )+
  #"darkorchid3", "firebrick3", "chartreuse3", "chartreuse4", "turquoise", "deepskyblue4"
  ggplot2::xlab(
    "SNP position"
  ) + 
  ggplot2::ylab(
    "Logged P-value"
  ) + 
  ggplot2::scale_x_continuous(
    breaks = seq(0,17000, 1000)
  ) +
  ggplot2::ylim(c(0, -100)
  ) +
  ggplot2::ylab(
    "Logged P-value"
  ) +
  ggplot2::theme(
    legend.position = "bottom", 
    axis.line = element_line(
      linewidth = 1, 
      colour = "black"
    ), 
    plot.background = element_rect(
      fill = "snow3"
    ), 
    panel.grid = element_line(
      colour = "snow3", 
      linetype = "dotdash"
    ),
    panel.grid.major.x = element_line(
      colour = "snow4", 
      linetype = "dashed"
    ), 
    legend.background = element_rect(
      fill = "snow3"
    )
  )
```

#### Reading in Gene Positions

Taking csv from MITOMAP

```{r}
mt_loci_pos <- read.csv(
  file = "~/Documents/data/lhf_d/GenomeLoci_MITOMAP_Foswiki.csv"
)

unique(mt_loci_pos$Shorthand)

mt_gene <- c("ATPase6", "ATPase8", "COI", "COII", "COIII", "Cytb", "ND1", 
             "ND2", "ND3", "ND4", "ND4L", "ND5", "ND6", "16S", "12S")

mt_gene_pos <- mt_loci_pos[mt_loci_pos$Shorthand %in% c(mt_gene),]

unique(mt_gene_pos$Shorthand)

mt_classification <- data.frame(
  shorthand = unique(mt_gene_pos$Shorthand),
  complex = c("C.V", "C.V", "C.IV", "C.IV", "C.IV", "C.III", "CI", "CI", "CI", "CI", "CI", "CI", "CI"   , "12S", "16S")
)

mt_gene_pos <- dplyr::left_join(
  x = mt_gene_pos,
  y = mt_classification, 
  by = join_by(Shorthand == shorthand)
) 

```

#### SNP loci distributions

This section is looking at which significant SNPs fall in each of the loci types
(Coding, Non-coding, tRNA).

Creating the combined tibble:

```{r combining_loci_sig}

anova_scores_ <- anova_scores

anova_scores_$snp_pos <- as.numeric(anova_scores_$snp_pos)

mt_loci_pos$Starting <- as.numeric(mt_loci_pos$Starting)
mt_loci_pos$Ending <- as.numeric(mt_loci_pos$Ending)

sig_loci_pos <- dplyr::left_join(
  x = anova_scores_, 
  y = mt_loci_pos, 
  by = join_by(snp_pos >= Starting, snp_pos <= Ending)
)

sig_loci_pos$classification <- sig_loci_pos$Shorthand

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "CR.*", 
  replacement = "CR"
)

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "12S|16S", 
  replacement = "rRNA"
)

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "ND.*", 
  replacement = "C_I"
)

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "CO.*", 
  replacement = "C_IV"
)

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "Cytb", 
  replacement = "C_III"
)

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "ATP.*", 
  replacement = "C_V"
)

sig_loci_pos$classification <- stringi::stri_replace_all(
  str = sig_loci_pos$classification, 
  regex = "NC.*", 
  replacement = "NC"
)

tRNA_pos <- sig_loci_pos %>%
  dplyr::filter(
    base::grepl(
      "tRNA", 
       Description
    )
  ) 

tRNA_pos$classification <- "tRNA"

sig_loci_pos <- sig_loci_pos %>%
  dplyr::filter(
    !base::grepl(
      "tRNA", 
       Description
    )
  ) 

sig_loci_pos <- rbind(sig_loci_pos, tRNA_pos)


unique(sig_loci_pos$classification)  

```
