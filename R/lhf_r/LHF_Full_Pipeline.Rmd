---
title: "LHF Analysis Pipeline"
author: "FiG-T"
data: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
    toc: yes
toc: TRUE
editor_options:
  markdown:
    wrap: 80
  chunk_output_type: inline
---

# Introduction

This markdown document is a guide to the formatting and analysis used on the
"Low Hanging Fruit" (LHF) project. The key aim of this project is to identify
any mitochondrial SNPs whose frequency correlates with (and is accurately model
by) climatic or environmental variables at the global scale. This requires (at
the broadest level): acquisition of the data, alignment of the sequences,
filtering of SNP variants, collating environmental variables, and analysis of
any correlations and the testing of models.

The specific code used to do this is not in it's entirety contained herein.
Instead this document may at some points reference other R scripts which have
further information on a particular aspect of the pipeline.

These scripts (and this markdown) can be found at
[<https://github.com/FiG-T/scripts/tree/main/R/lhf_r>].

## Overview

The following contains a list of steps in order of use:

1.  Download Genbank files from NCBI
2.  Extract relevant information from Genbank files (python)
3.  Convert to FASTA format
4.  Multiple sequence alignment (and filtering)
5.  Converting to VCF format (python)
6.  Filtering VCF files (bash)
7.  Checking allele frequency counts
8.  Collating environmental data
9.  Creating metadata files
10. Visualising genetic variation (inc PCAs)
11. Calculating population allele frequencies & Fst values
12. Creating Trees

## Downloading GenBank files

This section is taken directly from lhf_entrez_download_local.R (available on
the GitHub), copied in here for completeness.

As this project requires location data (which is not automatically included on
NCBI entries), the full GenBank files need to be acquired for all the relevant
genomes. This is achieved here through the use of the 'rentrez' package and API
which allows for the direct download of files from the NCBI server into R. (A
Python version is also available.) The default number of requests allowed by
NCBI is 3 per second - for large requests this can dramatically slow down the
processing speed. This limit can be increased to 10/s by registering for an NCBI
account and linking a personal API key.

```{r rentrez_setup, include=FALSE}

# install if needed:
# install.packages("rentrez")
library(rentrez)

# link personal API key
set_entrez_key("f5ff2d14bbc152047694a55253157602b507")
# set the key for each session
# use your own personal key
```

Once the rentrez package and API has been set up, NCBI can be queried directly.
The following search term was used here (in concordance with the term used by
the MITOMAP website):

```{r rentrez_search, include=FALSE}

mito_search <- entrez_search(
  db = "nucleotide",  # select the relevant database
  term = "(00000015400[SLEN] : 00000016600[SLEN]) AND Homo[Organism] AND mitochondrion
  [FILT] AND (15400[SLEN] : 17000[SLEN])",
  use_history = TRUE
) 

```

This search term defines the length (ensuring only full mtDNA sequences are
returned), the organism, as well as the organelle. Rather than downloading this
search result (which will impose an storage-related cutoff), the output is
stored directly on the NCBI server (\`use_history = TRUE\`).

To download the sequences the resulting search item can be used to fetch the
associated GenBank documents.

This loops through items in the search term (for the specified range) and
appends all the GenBank files into a singular text file. This was done in
multiple chunks for practicality reasons (it's easier and quicker to spot if
there was an error in smaller groups.

> Note: It is **strongly advised** that you keep the loop and chunk size
> small... Even if the code is running fine the NCBI server sometimes resets
> and/or time-outs the query. In this case it's better to know sooner rather
> than later.

```{r entrez_loop, include=FALSE}

# E.g.: for the final batch from 60001 to 64300: 

for( seq_start in seq(60001,64300,100)){
  # starting off with a small sample size
  recs <- entrez_fetch(  ----FAILSAFE: remove the comment at the start to execute 
    db ="nucleotide",  # the database to query
    web_history = mito_search$web_history, # the search term defined above
    #id = mito_search$ids,
    rettype ="gb",  # the type of file to return (GenBank)
    retmax = 100,   # the maximum number to return (this should match the loop size)
    retstart=seq_start)
  Sys.sleep(0.1)          # to ensure NCBI is not overloaded.
  cat(recs, file="/Users/finleythomas/Documents/data/lhf_d/mito_gb_60-64_.txt", append=TRUE)
  cat(seq_start + 99, "GenBank files downloaded\r")  # this helps to check progress. 
}

```

The end result of this process is a group of .txt files that contain a very
large number of concatenated GenBank files. The relevant information is
extracted from these in the following stages.

## Converting to FASTA format

#### Extracting information from GenBank flies

The following python code was created (with significant help from chatGBT) to
extract information from the downloaded text files and convert into a table
suitable for further data processing. It is available on the GitHub
(extract_gb_info_v3.6.py).

> Fail-safes (in the form of commented lines) have been added here to prevent
> the script from accidentally running and overwriting current files)

```{python, include = FALSE}

import sys
import re

if len(sys.argv) != 2:
    print("Usage: python extract_genbank_info.py input_file")
    sys.exit()

# input_file = sys.argv[1]
# output_file = "/Users/finleythomas/Documents/data/lhf_d/genbank_output_60-64.tsv"

with open(input_file, "r") as f:
    with open(output_file, "w") as out:
        out.write("Accession\tOrganism\tCountry\tNote\tCell_Line\tHaplogroup\tSequence\n")
        accession = ""
        organism = ""
        country = ""
        note = ""
        cell_line = ""
        haplogroup = ""
        sequence = ""
        in_features = False
        in_source = False
        for line in f:
            line = line.strip()
            if line.startswith("ACCESSION"):
                accession = line.split()[1]
            elif line.startswith("  ORGANISM") or line.startswith("ORGANISM"):
                organism = line.split("  ")[-1].strip()
            elif line.startswith("  /country") or line.startswith("country"):
                country = line.split("=")[-1].strip().strip('"')
            elif line.startswith("  /note") or line.startswith("note"):
                note = line.split("=")[-1].strip().strip('"')
            elif line.startswith("  /cell_line") or line.startswith("/cell_line"):
                cell_line = line.split("=")[-1].strip().strip('"')
            elif line.startswith("SOURCE"):
                in_source = True
            elif line.startswith("FEATURES"):
                in_features = True
            elif line.startswith("ORIGIN"):
                sequence_lines = []
                for next_line in f:
                    if next_line.startswith("//"):
                        break
                    sequence_lines.append(next_line.strip())
                sequence = ''.join(sequence_lines)
                sequence = re.sub(r'\d', '', sequence)
                out.write(accession + "\t" + organism + "\t" + country + "\t" + note + "\t" + #cell_line + "\t" + haplogroup + "\t" + sequence + "\n")
                accession = ""
                organism = ""
                country = ""
                note = ""
                cell_line = ""
                haplogroup = ""
                sequence = ""
                in_features = False
            elif in_features and "/country=" in line:
                country = line.split("=")[-1].strip().strip('"')
            elif in_features and "/note=" in line and not note:
                note = line.split("=")[-1].strip().strip('"')
            elif in_features and "/cell_line=" in line:
                cell_line = line.split("=")[-1].strip().strip('"')
            elif in_source and "/haplogroup=" in line:
                haplogroup = line.split("=")[-1].strip().strip('"')
            elif line.startswith("//"):
                in_source = False
        if accession:
            out.write(accession + "\t" + organism + "\t" + country + "\t" + note + "\t" + cell_line + "\t" + haplogroup + "\t" + sequence + "\n")

print("Extraction complete. Results saved to " + output_file + ".")

```

#### Processing extracted GenBank file

As country names need to be isolated from the records, these need to be imported
into the R workspace.

```{r country_info, include= FALSE}


```

## Converting to VCF format

> ***aln .FASTA -\> .vcf***

Here I used anaconda cloud as I was unable to install snp-sites (a python
bioconda package) locally (for unknown and undesipherable reasons).

```{python, include = FALSE, echo = TRUE}

# list available conda environments
conda env list
  # "*" denotes the active environment

# Activate the new environment (or existing one)
`conda activate <environment_name>`

# Install snp-sites 
`conda install -c bioconda snp-sites`

# Upload fasta file to cloud terminal, then: 

# Run snp-sites (changing names as required)
`snp-sites -v -o snp_sites_msa_converted_09_2023.vcf aln_mafft_incRef_2_09_2023.fasta`
```

## Filtering the VCF

The resulting VCF produced above will include any variable site in the
alignment. Many of these will not meet our cutoff criteria (e.g those likely
caused by mapping or sequencing errors) and thus must be removed before any
genetic analysis is attempted. The **easiest way to achieve this is to use
VCFtools**.

VCFtools is a bash program that can be run on the command line (and installed on
MAC iOS via homebrew - `brew install vcftools` ).

For this analysis the reference genome is removed (to avoid duplication effects)
and the minor allele frequency is set to 0.005. This frequency is lower that
standard (0.05) however this filtering is designed to remove spurious mapping
artefacts: a MAF of 0.005 from a data set of 23,807 individuals equates to a
minimum count of 119 - this is unlikely to be reached by chance alone.

```{bash vcftools_filtering, include = FALSE, echo = TRUE}

# set the working directory to the folder with vcf files
cd <path/to/vcfs

# remove the reference individual: 
vcftools --vcf snp_sites_msa_filtered_converted_09_2023.vcf --remove-indv NC_012920.1 --recode --out snp_sites_filtered_vcf_noRef_092023

# create a new VCF with a MAF of 0.005 
vcftools --vcf snp_sites_filtered_vcf_noRef_092023.recode.vcf --maf 0.005 --recode --out snp_sites_filtered_vcf_maf0.005_noRef_092023

# (This was repeated for 0.05 and 0.001 cutoffs)
```

> **This results in a VCF file with 442 sites (of an original 7254) and 23,790
> individuals. This is the file that will be used going forwards.**

### Generating allele frequency information

VCFtools can also be used to calculate allele frequencies and missing data
proportions. These can be used as a useful sanity check for the data.

*Note: the maximum number of alleles here is 3 (not the typical 2) - each of
these will still meet the minimum allele frequency specified above.*

```{bash vcftools_checks, include = FALSE, echo = TRUE}

# calculate the allele frequencies:
vcftools --vcf snp_sites_filtered_vcf_maf0.005_092023.recode.vcf --freq --out ./lhf_allele_freq --max-alleles 3

# calculate the proportion of missing data (where no informative information is available)

# per site: 
vcftools --vcf snp_sites_filtered_vcf_maf0.005_092023.recode.vcf --missing-site --out ./missing_site

# per individual
vcftools --vcf snp_sites_filtered_vcf_maf0.005_092023.recode.vcf --missing-indv --out ./missing_indv
```

## VCF checks & additional processing

> **For additional information on this section, see vcf_processing.R**

To visualise the 'missingness' and allele frequency data generated by vcf tools
[vcftools_checks] we need to load the data into R.

### Allele frequencies

The distribution of the minor allele frequencies is a good check of the quality
of the data, ideally a smooth curve should be observed (highest at very low
frequencies).

```{r check_allele_freq, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)

# load in the data
var_freq <- readr::read_delim(
  file = "~/Documents/data/lhf_d/vcf/lhf_allele_freq_nobase.frq", 
  delim = "\t", 
  col_names = c("chr", "pos", "nalleles", "nchr", "a1", "a2", "a3"), 
  skip = 1
)

# calculate the frequency of the minor allele...
var_freq$maf <- var_freq %>% 
  dplyr::select(a1, a2, a3) %>% 
  apply(1, function(z) min(z, na.rm = TRUE))

# plot the distributionn of alleles
ggplot2::ggplot(
  data = var_freq, 
  mapping = ggplot2::aes(maf)
) + 
  ggplot2::geom_histogram(
  bins = 99

) +
  ggplot2::geom_density(
    fill = "orchid3", 
    colour = "black", 
    alpha = 0.75
) +
ggplot2::xlab(
  "Minor Allele Frequency"
) +
ggplot2::ylab(
  "Density"
)
```

This shows some variation from what is typically expected - it is unclear what
exactly is causing this... (I suspect it could be related the the frequencies of
specific mitochondrial haplotypes in specific countries). Looking at the alleles
responsible for the peaks they do not appear upon visual inspection to be
associated with a specific set of countries.

### Missingness

Similar to allele frequencies, it is good to be aware of exactly what data you
may be missing in the data set...

```{r check_missingness,  echo=FALSE, warning=FALSE, message=FALSE}

# missing data per site:
var_miss <- readr::read_delim(
  "~/Documents/data/lhf_d/vcf/missing_site.lmiss", 
  delim = "\t",
  col_names = c(
    "chr", "pos", "nchr", "nfiltered", "nmiss", "fmiss"
  ), 
  skip = 1)

# plot missingness
ggplot2::ggplot(
  data = var_miss, 
  mapping = ggplot2::aes(fmiss)
) + 
ggplot2::geom_density(
  fill = "orchid3", 
  colour = "black", 
  alpha = 0.3
) + 
ggplot2::xlab(
  "Frequency of missing allele"
) +
ggplot2::ylab(
  "Density"
)

# missing data per individual: 
var_miss <- readr::read_delim(
  "~/Documents/data/lhf_d/vcf/missing_indv.imiss", 
  delim = "\t",
  col_names = c(
    "chr", "pos", "nchr", "nfiltered", "nmiss", "nmiss...5"
  ), 
  skip = 1)

# plot missingness
ggplot2::ggplot(
  data = var_miss, 
  mapping = ggplot2::aes(nmiss...5)
) + 
ggplot2::geom_density(
  fill = "orchid3", 
  colour = "black", 
  alpha = 0.3
) + 
ggplot2::xlab(
  "Frequency of missing data per individual"
) +
ggplot2::ylab(
  "Density"
)
```

Both these plots show that there is no missing data in our VCF file.

### Collecting sample names

During the multiple sequence alignment filtering steps we have removed a number
of individuals (samples) from the original data. For the metadata files (created
later) we need to ensure that the list of samples names with the associated
environmental data *exactly matches* the sample names in the VCF file.

The list of sample names is best queried using **BCFtools**, another command
line tool capable of quickly dealing with VCF (and bcf) files. (It can also be
downloaded using homebrew on Mac OS)

```{bash bcftools_sample_names, include=FALSE, echo=TRUE}

# navigate to the correct directory:
cd path/to/vcfs

# run bcftools query
bcftools query -l snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf > lhf_filtered_maf0.005_sampleNames.txt

```

Problematic names should have been removed at an earlier stage (but some may
have slipped through the net). To avoid any further issues some name
substitutions can be completed.

```{r sample_names, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

# import bcftools query into R
lhf_names <- readr::read_delim(
  file = "~/Documents/data/lhf_d/vcf/lhf_filtered_maf0.005_sampleNames.txt", 
  delim = "/t", 
  col_names = "samples"
)

# separate out names 
lhf_names <- tidyr::separate(
  data = lhf_names, 
  col = samples, 
  into = c("acc", "a", "b", "c", "d" , "e", "f"), 
  sep = "_"
) 
# combine country words into a single column
lhf_names <- tidyr::unite(
  data = lhf_names, 
  col = "country", 
  a, b, c, d, e , f, 
  sep = " ", 
  na.rm = TRUE
)

# replace names of problem countries
lhf_names$country <- gsub(
  pattern = "USA", 
  replacement = "United States",
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "Great Britain", 
  replacement = "United Kingdom",
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "Czech Republic", 
  replacement = "Czechia", 
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "RussiaRussia", 
  replacement = "Russia", 
  x = lhf_names$country
)
lhf_names$country <- gsub(
  pattern = "JapanJapan", 
  replacement = "Japan", 
  x = lhf_names$country
)

# convert to a table column for later:
lhf_names_col <- as.data.frame(lhf_names)

# select unique variables
lhf_names <- unique(lhf_names$country)
lhf_names <- sort(lhf_names)


```

This results in the following list of countries:

```{r country_list, echo=FALSE}
lhf_names
```

## Extracting Climate Variables

Climate data can be downloaded from the
[WorldClim](%5B%5Bhttps://www.worldclim.org/data/bioclim.html%5D%5D(https://www.worldclim.org/data/bioclim.html%5D))
online database; this is derived from remote sensing (satellite) measurements
and covers the years 1970 to 2020 (as of September 2023). A 'historical' dataset
can be obtained which excludes years post 2000 however this is harder to
download (and I was not sure if this was a biologically coherent thing to do).

This section requires extensive use of the `terra` and `geodata` R packages
(which are the successors to `raster` ).

### Downloading the data

```{r terra_install, echo=FALSE}
# if required, download these packages: 
if (!require("geodata")) install.packages("geodata")
if (!require("tidyterra")) install.packages("tidyterra")
```

The data can be downloaded directly using the geodata package:

It is possible to select the resolution of the data (see the WorldClim site for
more details) - this relates to the frequency of the measurements (and thus the
size of each square for each measurement). A resolution of 2.5 minutes (selected
here) roughly equates to each raster square equaling 11 square km.

```{r global_bio, echo=TRUE}
global_bio <- geodata::worldclim_global(
  var = "bio",  # which variable to download
  res = 2.5,  
  path = tempdir(), 
  version = "2.1"
)
# note the output of this is a SpatVector, a S4 class object specific to the terra package.
```

Selecting the **bio** variable downloads a large table of 15 biologically
relevant environmental covariates, for downstream analysis we will un-select
some of the precipitation-based measures. The remaining measures are:

|       |                                            |
|-------|--------------------------------------------|
| BIO1  | Annual mean temperature                    |
| BIO2  | Mean diurnal range                         |
| BIO3  | Isothermality (BIO2 x BIO7)                |
| BIO4  | Temperature Seasonality                    |
| BIO5  | Maximum temperature (of the warmest month) |
| BIO6  | Minimum temperature (of the coldest month) |
| BIO7  | Temperature annual range                   |
| BIO12 | Annual precipitation                       |
| BIO15 | Precipitation Seasonality                  |

```{r select_env_covariates, echo=FALSE}

# select the relevant bio columns
env_covariates <- global_bio[[c(1,2,3,4,5,6,7,12,15)]]

names(env_covariates) <- c(
  "tmp_yr", "tmp_range_drl", "isotherm", "tmp_seasonality", "tmp_max", "tmp_min", 
  "tmp_range_yr", "precip_yr", "precip_seasonality"
)
```

### Generating centroid coordinates

A map of the world with the country boundaries can further be downloaded and
used to generate the centroid coordinates of each country that is present in the
LHF dataset.

```{r world_map, echo=FALSE, message=FALSE}

# download the full map coordinates
world_coord <- geodata::world(
  resolution = 3,  
  # 1 = highest resolution, 5 = lowest resolution
  level = 0, 
  path = tempdir()
)

# generate a dataframe with country names: 
world_coord_df <- as.data.frame(world_coord$NAME_0)

# calculate the centroid coordinates: 
country_centroids <- terra::centroids(
  x = world_coord, 
  inside = TRUE # this will guarantee the point falls within the polygon, but it may not be 
                # the true centroid.
)

# select only country that are relevant to LHF (using the country list generated above)
lhf_centroids <- country_centroids[country_centroids$NAME_0 %in% c(lhf_names),]

# to convert the SpatVector to a dataframe:
# collect country centriod coordinates
world_centroid <- terra::geom(country_centroids)
```

These centroid coordinates can be plotted over a blank map:

```{r centroids_blank_map, echo=FALSE}

# plot the underlying map
plot(world_coord, 
  col = 'snow3', 
  border = "snow4"
)
# overlay the centroid coordinates
terra::points(
  lhf_centroids, 
  col = "darkorchid3", 
  pch = 20, 
  cex = 1
)
```

Or over a map with an environmental covariate shown: (Annual temperature range
shown here)

```{r map_env_covariates, message=FALSE}
bio <- 7  # select the desired environmental trait
plot(
  env_covariates[[bio]], 
  main = names(env_covariates)[bio]
  ) # plot the environmental data 
# add the centroid points
terra::points(
  lhf_centroids, 
  col = "purple", 
  pch = 20, 
  cex = 1
)
```

To collect ISO information on the full list of country codes:

```{r country_codes, echo=FALSE}
country_codes <- geodata::country_codes(
  query = NULL
)
# For consistency...
country_codes$NAME <- gsub(
  pattern = "Czech Republic", 
  replacement = "Czechia", 
  x = country_codes$NAME
)
```

## Generating metadata files

### Extracting data

The first step in creating the metadata files is to extract the relevant data
from the global dataset, there are multiple different ways to do this. It is
hard to determine which of these is best to use, and there are a few pros and
cons for each.

The two methods written out here are: (or at least they will be eventually..)

-   extracting data from country centroid positions

-   extracting mean values for a polygon of each country

#### To extract mean values by region (option 2):

```{r mean_extracted, echo=TRUE}
# extract the mean values for each of the countries
lhf_env <-   terra::extract(
  env_covariates, 
  world_coord, 
  mean,
  na.rm = TRUE, 
  weights = TRUE
)
```

To combine this with the country names, coordinates, and sample IDs:

```{r meta2_processing, echo=TRUE}

# combine with all country names:
lhf_env. <- base::cbind(
  world_coord_df, 
  lhf_env
)

# to add country centroids:
lhf_env. <- base::cbind(
  lhf_env., 
  world_centroid
)

# reorder columns (and remove unwanted):
lhf_env. <- lhf_env.[,c(1,14,15,3:11)]

# rename wordy column names:
names(lhf_env.)[1:3] <- c("country", "long", "lat")

# still causing issues...
lhf_env.$country <- gsub(
  pattern = "Czech Republic", 
  replacement = "Czechia", 
  x = lhf_env.$country
)
```

```{r meta2_file_creation, echo=FALSE}

# join the lhf_env table to the list of names 
  # as left_join is used only countries matching 
lhf_meta2 <- dplyr::left_join(
  x = lhf_names_col, 
  y = lhf_env.,
  by = dplyr::join_by(country)
)

# add in regions and continents: 
lhf_meta2 <- dplyr::left_join(
  x = lhf_meta2, 
  y = country_codes, # quereied above
  by = dplyr::join_by("country" == "NAME")
)

# reorder and remove unwanted cols: 
lhf_meta2 <- lhf_meta2[,c(1:4,19, 14, 15, 20, 22, 5:13)]
```

```{r meta2_format, echo=FALSE}
# unite first two columns to reform sample names: 
lhf_meta2 <- tidyr::unite(
  data = lhf_meta2, 
  col = "samples", 
  acc, country,
  sep = "_", 
  na.rm = TRUE, 
  remove = FALSE
)

# remove spaces from sample names: 
lhf_meta2$samples <- gsub(
  pattern = " ", 
  replacement = "_", 
  x = lhf_meta2$samples
)

# change iso column names (to remove capital letters)
names(lhf_meta2)[6:9] <- c("sovereign", "iso3","iso2", "subcontinent")

```

Save this meta file (so that you do not need to rerun this for every script).
Feather format was chosen here as it allows for extremely rapid download and
upload into R (and Python).

```{r meta2_feather, echo=TRUE}

# write to alternate feather file 
feather::write_feather(
  x = lhf_meta2, 
  path = "~/Documents/data/lhf_d/lhf_meta2_maf0.005_data_11_2023.feather"
  # change this path to path the desired destination
)

# save file grouped by country: 
feather::write_feather(
  x = lhf_env., 
  path = "~/Documents/data/lhf_d/lhf_country_metadata_maf0.005_11_2023.feather"
  # change this path to path the desired destination
)

```

### Filtering countries by N

As many of the following stages use measures of allele frequencies (AF) or
heterozygousity (Hs) we want to make sure we only include populations where
there are a sufficient number of individuals to accurately calculate these
values.

> **The cutoff selected here is 20 individuals.**

This is twice the number used by Harrison/ the TRMP8 paper (as they were looking
at autosomes and thus had 2 counts per individual).

```{r country_N, echo=FALSE}
library(dplyr)

# group by country and summarise: 
country_N <- lhf_meta2 %>%  # generated above
  group_by(country) %>%
  summarise(
    N = n()
  )

# filter out countries with fewer than 20
country_N <- country_N %>%
  filter(N >= 20)  # define the cutoff value. 

# convert remaining countries to a list 
country_N <- c(country_N$country)

```

## Importing mtDNA loci

## Visualising Genetic Variation

For further information on this script, see **vcf_freq_env_visualisation.R**.

### Setup

This section utilises the *adegenet*, parallel and *mappplot* packages. These
may need to be downloaded:

```{r adegent_install, echo=FALSE}
if (!require("adegenet")) install.packages("adegenet")
if (!require("parallel")) install.packages("parallel")
if (!require("mapplots")) install.packages("mapplots")

# the following libraries are also required for this section: 
library(vcfR)
library(feather)
library(adegenet)
library(parallel)
library(ggplot2)
library(tidyr)
library(dplyr)
library(maps)
library(mapplots)
```

#### Data required:

Two files are required for this section: the filtered VCF and the associated
meta data file (see above for how to make these).

> **The row names in the meta file must *exactly match* the sample names in the
> VCF.**

```{r upload_paired_files, echo=TRUE}

# read in vcf file 
lhf_vcf <- vcfR::read.vcfR(
  file = "~/Documents/data/lhf_d/vcf/snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf", 
  # this file is the same as the one used for vcf processing
  verbose = TRUE
)

# note the second meta data file is used here (with country-wide averages)
lhf_meta2 <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_meta2_0.005_data_11_2023.feather"
)

```

#### Converting to a Genlight object

The adegenet (and other packages) require the vcf data to be in a "Genlight"
format (this is another S4 data class). To convert the vcfR to a genlight
object:

```{r create_genlight, echo=TRUE}

# this can be done using the vcfR package:
vcf.gl <- vcfR::vcfR2genlight(
  x = lhf_vcf
) # this will omit any non-biallelic sites
```

```{r genlight_meta, echo=TRUE}

#  Specify the country IDs
adegenet::pop(vcf.gl) <- lhf_meta2$country

adegenet::ploidy(vcf.gl) <- 1 # is this a correct assumption?

# view genlight object 
vcf.gl

```

In this example *country* information is specified to the 'population'
variable - this could also be continent or subcontinent information (or any
other defining variable in the metadata file).

We can use this genlight object to run Principal Component Analysis and
calculate the allele frequencies.

### PCA plots

To generate the pca scores and eigenvalues we can use the *adegenet* package...

***Note: This takes a very large (many hours) amount of time to run (given the
very large number of sequences).***

```{r create_pca, echo=FALSE, inlcude = FALSE}

vcf.pca <- adegenet::glPca(
  # x = vcf.gl,  # FAILSAFE-remove initial hashtag
  nf = 4,          # how many PCs to retain
  parallel = TRUE, # unless specified otherwise, the max number of available cores will be used. 
)

```

Once generated, we can convert the PCA scores into a tibble (and save this).

```{r format_pca_scores, echo=FALSE, include=FALSE}

# convert to a tibble
vcf.pca.scores <- tidyr::as_tibble(vcf.pca$scores)

# as the pca takes a long time to run ... save these scores... 
feather::write_feather(
  x = vcf.pca.scores, 
  path = "~/Documents/data/lhf_d/lhf_pca_scores_0.005.feather"
)

```

Once the PCA scores have been generated (and saved) once, they can be read in
with the following code:

```{r read_pca_scores, echo=FALSE}

vcf.pca.scores <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_pca_scores_0.005.feather"
)
```

We can add information from the metadata file to the PCA scores:

```{r pca_meta2, echo=TRUE}

# add in population data (country used as an example here)
vcf.pca.scores$country <- lhf_meta2$country
vcf.pca.scores$subcontinent <- lhf_meta2$subcontinent
vcf.pca.scores$continent <- lhf_meta2$continent


# add in env data (edit to chose, this could be any column in the metadata file)
vcf.pca.scores$lat <- lhf_meta2$lat

```

As a sanity check/ precaution, we can plot the PCs vs the proportion of the
variance that they explain, as well as calculating the exact proportion for a
specific component:

```{r pca_contributions, echo=TRUE}

# to create the overall plot:
graphics::barplot(
  vcf.pca$eig / sum(vcf.pca$eig) *100,
  space = 0.1,
  fill = "darkseagreen"
)

# calculate the sum of all eigenvalues
 eig.total <- sum(vcf.pca$eig)

# check the % for each of the primary PCs
for (i in 1:4){
  print(i)
  print(
    formatC(
      head(vcf.pca$eig)[i]/eig.total * 100
    )
  )
 # where i is the number of the PC you wish to show...
}
```

#### Creating the plots:

The vcf scores above can be plotted to show the distribution of samples, these
an be coloured/shaped by any of the variables in the meta file which have been
added to the PCA scores tibble (see above: chunk pca_meta2).

Given the very large number of countries in the data, colouring by this value is
not advised (continents gives a much clearer overview).

```{r pca_plots, echo=TRUE}

# plotting PC1 vs PC2
 ggplot2::ggplot(
   data = vcf.pca.scores, 
   mapping = ggplot2::aes(
     x = PC1, 
     y = PC2, 
     col = continent
   )
) +
ggplot2::geom_point(
)

# plotting PC3 vs PC4
 ggplot2::ggplot(
   data = vcf.pca.scores, 
   mapping = ggplot2::aes(
     x = PC3, 
     y = PC4, 
     col = continent
   )
) +
ggplot2::geom_point(
)
```

#### Identify the SNPs with greatest influence:

We can use PCA weighting as a proxy for which SNPs have the highest influence
over genetic variation.

```{r pca_weightings, echo=FALSE}

# select PCA loading values for the 4 Principal Components
snp_loadings <- tidyr::as_tibble(
  data.frame(
  vcf.gl@loc.names, 
  vcf.pca$loadings[,1:4]
  )
)

# rank the SNPs by their weighting for PC1 (Axis 1)
snp_loadings <- dplyr::arrange(
  .data = snp_loadings, 
  desc(Axis1)
)
```

```{r snp_loadings, echo=FALSE}
head(snp_loadings)
```

\-\-\-\-\-- NOTE: TODO - SAVE THESE LOADINGS/ PCA ------------

## Population Allele Frequencies

> After getting very muddled in this section I decided it needed a complete
> re-write: what follows is the product of this.

Allele frequencies are a focal point of this study and will be used to determine
whether environmental variables likely exert a selective pressure on mtDNA.

### Setup

```{r load_AF_build_data, echo=FALSE}

library(dplyr)

# start with the vcf file: 
lhf_vcf <- vcfR::read.vcfR(
  file = "~/Documents/data/lhf_d/vcf/snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf", 
  # this file is the same as the one used for vcf processing
  verbose = TRUE
)

# add in the metadata 
# note the second meta data file is used here (with country-wide averages)
lhf_meta2 <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_meta2_0.005_data_11_2023.feather"
)

# plus country grouped values: 
lhf_env. <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_country_metadata_maf0.005_11_2023.feather"
  # change this path to path the desired destination
)
```

For downstream analysis the data is converted to GENID and GENPOP objects. Both
are S4 class objects (similar to genlight objects used above) that are suitable
for working with large scale genetic data that is separated at the individual
(genid) or population (genpop) level - both are designed to work with *adegenet*
and *hierfstat* packages.

```{r genid_create, echo =FALSE}

# convert to genid
lhf_genid <- vcfR::vcfR2genind(
  lhf_vcf
)

# add in country data
adegenet::pop(lhf_genid) <- lhf_meta2$country

# check that population data was added
head(lhf_genid@pop)

```

The GENID object can be filtered to only include countries that have more that
the cutoff (20) number of individuals (see 'Filtering countries by N').

```{r genid_filter, echo=FALSE}
lhf_genid <- lhf_genid[lhf_genid@pop %in% country_N]
```

This can be converted to a GENPOP object, grouping individuals by country:

```{r create_genpop, echo=FALSE}
lhf_genpop <- adegenet::genind2genpop(lhf_genid)
```

### Calculating Population Allele Frequencies

To generate allele frequencies the `makefreq()` function from *adegenet* was
used.

> Note: The assumptions of this process are a bit unknown - does it factoring in
> a measure of Hs? Is it fully applicable to mtDNA?

```{r calculate_AF}

# generate frequencies
lhf_AF <- adegenet::makefreq(
  x = lhf_genpop, # using the genpop object
  missing = NA
)
# NOTE: this data is separated by allele NOT loci... 

# convert to a tibble: 
lhf_AF <- tidyr::as_tibble(
  x = lhf_AF, 
  rownames = "pop" # tranfer the rownames into a new column
)
```

Once generated, the AF data requires considerable formatting:

```{r AF_formatting, echo = FALSE, warning=FALSE}

# convert into a long format (to help deal with loci & alleles)
lhf_AF <- tidyr::pivot_longer(
  data = lhf_AF,
  cols = -1,
  names_to = "allele", 
  values_to = "a_freq"
)

# remove the "1_" prefix on SNP positions
lhf_AF$allele <- gsub(
  pattern = "1_", 
  replacement = "", 
  x = lhf_AF$allele
)

# replace special characters (causes issues later on)
lhf_AF$allele <- gsub(
  pattern = "\\.", 
  replacement = "_", 
  x = lhf_AF$allele
)

# separate out the alleles 
lhf_AF <- tidyr::separate_wider_delim(
  data = lhf_AF, 
  cols = allele, 
  delim = "_", 
  names = c("pos", "allele")
)

# convert into a wider format (col per allele) 
lhf_AF <- tidyr::pivot_wider(
  data = lhf_AF, 
  names_from = "allele", 
  values_from = "a_freq"
)

# rename columns: 
colnames(lhf_AF) <- c(
  "pop", "pos", 
  "a1_freq", "a2_freq", "a3_freq", "a4_freq"
)

## To combine with environmental data: 
# join with country summaries
allele_freq_location <- dplyr::left_join(
  x = lhf_AF, 
  y = lhf_env., 
  by = dplyr::join_by("pop" == "country")
)

# Add in naming info
allele_freq_location <- allele_freq_location <- dplyr::left_join(
  x = allele_freq_location, 
  y = country_codes[,c(1:3,8,10)], 
  by = dplyr::join_by("pop" == "NAME")
)
```

The resulting table contains rows of data for each position x country
combination, with columns for the frequency of each of the possible alleles
(typically 2 but up to 4), as well as all the associated environmental data for
each country.

```{r head_lhf_AF}
head(allele_freq_location)
```

**Save this file:**

```{r AF_download}

feather::write_feather(
  x = allele_freq_location, 
  path = "~/Documents/data/lhf_d/lhf_country_allele_freq_maf0.005.feather"
)

```

### Plotting Allele Frequencies

#### Pie-chart maps

The allele frequencies can be plotted ontop of the global maps:

Pick a allele of interest:

> POS_9607 used here as it contributes most to the PCA variance.

```{r allele_freq_select, echo=FALSE}
allele_freq_select <- allele_freq_location[allele_freq_location$pos == "15215",]
# 193
```

```{r plot_AF}

# using coordinates gathered earlier: 
plot(
  world_coord, 
  col = 'snow3', 
  border = "snow3"
)

for (i in 1:nrow(allele_freq_select)) {
  mapplots::add.pie(
    z = c(
      allele_freq_select$a1_freq[i], 
      allele_freq_select$a2_freq[i], 
      allele_freq_select$a3_freq[i],
      allele_freq_select$a4_freq[i]
    ), 
    x = allele_freq_select$long[i], 
    y = allele_freq_select$lat[i], 
    radius = 3, 
    col = c(
      ggplot2::alpha("cadetblue3", 0.8), ggplot2::alpha("orchid2", 0.8),
      ggplot2::alpha("firebrick", 0.8), ggplot2::alpha("darkorchid3", 0.8)
    ), 
    labels = allele_freq_select$pos[i], 
    label.dist = 1.1, 
    cex = 0.5
  )
}

```

Instead of plotting globally on a map the values can also be shown on a
traditional axis.

\-\-\-- **How to add error bars to this?**

```{r allele_freq_points, include=TRUE, echo=FALSE}
ggplot2::ggplot(
  data = allele_freq_select, 
  mapping = ggplot2::aes(
    x =  reorder(x = pop, +a1_freq), # plot the bars in value order
    y = a1_freq, # value to plot
    colour = continent
  )
) + 
  ggplot2::geom_point(
    #stat = "identity"
  ) +
  ggplot2::scale_colour_manual(
    values = c(
      "deepskyblue4", "firebrick4", "darkorchid3", "chartreuse4", "turquoise",
  "chartreuse3"
    )
  ) +
  ggplot2::theme(
     axis.text.x = ggplot2::element_text(
       angle = 90,
       vjust = 0.5,
       hjust = 0.5,
       size = 5
       )
) +
ggplot2::ylim(c(
    min(allele_freq_select$a2_freq)-0.05, 1 # set the axis to be more precise
  )
) + 
ggplot2::xlab("Country"
) + 
ggplot2::ylab("Allele Frequency")
```

#### Allele frequency distributions 

One concern that I have with this data is that at each SNP there is an excess of
the (presumably) ancestral allele; this leads to extremely skewed allele
frequency distributions. Given the implications for parametric analysis (and the
potential effect on downstream model residuals) I think this is something worth
looking into (or at the very least considering).

```{r AF_wide_formatting, echo=FALSE}

# convert to wide format
lhf_AF_wide <- tidyr::pivot_wider(
  data = allele_freq_location[,-c(4,5,6)],  # change these col depending on the allele selected
  names_from = pos, 
  values_from = a1_freq, 
  names_prefix = "pos_"
)

# remove problem countries (from tree - see below)
problem_countries <- c("Canada", "Belarus", "Finland", "Netherlands")

##
## ----------------- ^^ move this filter higher up... 

lhf_AF_wide <- lhf_AF_wide %>%
  filter(! pop %in% problem_countries)

# remove columns with na only 
#lhf_AF_wide <- lhf_AF_wide %>%
#  select_if(~ !any(is.na(.)))

lhf_AF.df <- 0
lhf_AF.df <- as.data.frame(
  lhf_AF_wide
)

```

Looking at the distributions:

```{r AF_distributions, echo=TRUE}

hist(
  lhf_AF_wide$pos_4388, 
  col = "deepskyblue4", 
  breaks = 25
)

shapiro.test(lhf_AF_wide$pos_4388)
```

I have not run this extensively but for the SNPs checked there is always an
extreme skew/ inflation of 1 values. The following transformations were applied
(but with no success):

```{r AF_distribution_transformations, echo=FALSE}
# looking at 4388
hist(
  log10(lhf_AF_wide$pos_4388), 
  col = "deepskyblue4", 
  breaks = 25
)

hist(
  log(lhf_AF_wide$pos_4388), 
  col = "firebrick4", 
  breaks = 25
)

hist(
  sqrt(lhf_AF_wide$pos_4388), 
  col = "darkorchid4", 
  breaks = 25
)

hist(
  1/sqrt(lhf_AF_wide$pos_4388), 
  col = "orchid3", 
  breaks = 25
)


#shapiro.test(lhf_AF_wide$pos_4388)

```

## Calculating Heterozygosity

Calculating the heterozygosity by population is valuable for looking at the
genetic diversity across the mtDNA. (Plus this is what previous studies have
looked at.)

For the **Setup** - see the section in the 'Calculating Allele Frequencies'
section.

```{r pop_Hs, echo=TRUE}

pop <- as.factor(c(lhf_meta2$country))

pop_diffs <- vcfR::genetic_diff(
  vcf = lhf_vcf, 
  pops = pop
  # uses Nei's distance as standard
)
```

This data will require reformatting:

```{r pop_AF_formatting, echo=FALSE}

allele_Hs<- pop_diffs[,c(1:149)] # change ncol to fit the data (i.e: no. countries)

allele_Hs <- reshape2::melt(
  allele_Hs
)

names(allele_Hs)[3:4] <- c("location", "a_Hs") 

# remove erronous prefixes:
allele_Hs$location<- gsub(
  pattern = "Hs_", 
  replacement = "", 
  x = allele_Hs$location
)
allele_Hs$location <- gsub(
  pattern = "n_", 
  replacement = "", 
  x = allele_Hs$location
)

```

This data can be combined with mean coordinates (used for plotting). If
populations have been assigned at the country level 'lhf_centroids' can be used
as the coordinates for this. If subcontinent or continent is assigned to the
population data, mean coordinates have to be generated for these.

```{r continent_coord, echo=FALSE, message=FALSE}

# select continent/country/subcontinent as required
location_coords <- data.frame(
  lhf_meta2$country, lhf_meta2$lat, lhf_meta2$long, lhf_meta2$iso3
)

# requires dplyr to be loaded...
library(dplyr)
location_coords <- location_coords %>%
  group_by(lhf_meta2.country) %>%
  mutate(lat = mean(lhf_meta2.lat)) %>%
  mutate(long = mean(lhf_meta2.long))

location_coords <- location_coords[,-c(2,3)]
location_coords <- unique(location_coords)
names(location_coords)[c(1,2)] <- c("location", "iso3")

```

Once coordinates are available, these can be joined to the allele frequency
data:

```{r Hs_join_coord, echo=FALSE}

# if using continents: 
allele_Hs_location <- dplyr::left_join(
  x = allele_Hs, 
  y = location_coords, 
  by = join_by("location" == "location")
)

allele_Hs_location <- allele_Hs_location <- dplyr::left_join(
  x = allele_Hs_location, 
  y = country_codes[,c(1:3,8,10)], 
  by = dplyr::join_by("location" == "NAME")
)

allele_Hs_location <- allele_Hs_location[allele_Hs_location$location %in% c(country_N),]
# see section above on how to generate this list...

head(allele_Hs_location)
```

Save this file:

```{r AF_download}

feather::write_feather(
  x = allele_Hs_location, 
  path = "~/Documents/data/lhf_d/lhf_country_allele_Hs_maf0.005.feather"
)

```

### Plotting Heterozygosity 

Frequencies of specific alleles can also be plotted on a world map.

E.g.: For allele 9607 (which has the largest loadings on the PCA generated
above)

```{r select_allele, echo=TRUE}

# isolate the allele of interest: 
allele_Hs_select <- allele_Hs_location[allele_Hs_location$POS == "9607",]

```

```{r plot_af, echo=FALSE, message=FALSE, warning=FALSE}

# using coordinates gathered earlier: 
plot(
  world_coord, 
  col = 'snow3', 
  border = "snow3"
)

for (i in 1:nrow(allele_Hs_select)) {
  mapplots::add.pie(
    z = c(
      allele_Hs_select$a_Hs[i], 
      1- allele_Hs_select$a_Hs[i]
    ), 
    x = allele_Hs_select$long[i], 
    y = allele_Hs_select$lat[i], 
    radius = 3, 
    col = c(
      ggplot2::alpha("cadetblue3", 0.8), ggplot2::alpha("orchid2", 0.8)
    ), 
    labels = allele_Hs_select$iso3[i], 
    #label.dist = 1.1, 
    cex = 0.5
  )
}

```

**To plot the values on the axis...**

```{r Hs_point_plots, fig.cap="Heterozygosity Plot"}

ggplot2::ggplot(
  data = allele_Hs_select, 
  mapping = ggplot2::aes(
    x =  reorder(x = location, +a_Hs), # plot the bars in value order
    y = a_Hs, # value to plot
    fill = continent
  )
) + 
  ggplot2::geom_bar(
    stat = "identity"
  ) +
  ggplot2::scale_fill_manual(
    values = c(
      "deepskyblue4", "firebrick4", "darkorchid3", "chartreuse4", "turquoise",
  "chartreuse3"
    )
  ) +
  ggplot2::theme(
     axis.text.x = ggplot2::element_text(
       angle = 90,
       vjust = 0.5,
       hjust = 0.5,
       size = 5
       )
) +
ggplot2::ylim(c(
    min(allele_Hs_select$a_Hs)-0.05, 0.55 # set the axis to be more precise
  )
) + 
ggplot2::xlab("Country"
) + 
ggplot2::ylab("Heterozygousity")
```

To plot by continent:

```{r Hs_freq_continent, include=FALSE}

pop <- as.factor(c(lhf_meta2$subcontinent))

continent_diffs <- vcfR::genetic_diff(
  vcf = lhf_vcf, 
  pops = pop
  # uses Nei's distance as standard
)

continent_allele_Hs <- continent_diffs[,c(1:24)] # change ncol to fit the data (i.e: no. countries)

continent_allele_Hs <- reshape2::melt(
  continent_allele_Hs
)

names(continent_allele_Hs)[3:4] <- c("location", "a_Hs") 

# remove erronous prefixes:
continent_allele_Hs$location<- gsub(
  pattern = "Hs_", 
  replacement = "", 
  x = continent_allele_Hs$location
)
continent_allele_Hs$location <- gsub(
  pattern = "n_", 
  replacement = "", 
  x = continent_allele_Hs$location
)

# select continent/country/subcontinent as required
continent_coords <- data.frame(
  lhf_meta2$subcontinent, lhf_meta2$lat, lhf_meta2$long
)

# requires dplyr to be loaded...
continent_coords <- continent_coords %>%
  group_by(lhf_meta2.subcontinent) %>%
  mutate(lat = mean(lhf_meta2.lat)) %>%
  mutate(long = mean(lhf_meta2.long))

head(continent_coords)

continent_coords <- continent_coords[,-c(2,3)]
continent_coords <- unique(continent_coords)
names(continent_coords)[1] <- "location"

# if using continents: 
allele_Hs_continent <- dplyr::left_join(
  x = continent_allele_Hs, 
  y = continent_coords, 
  by = join_by("location" == "location")
)
```

```{r plot_Hs_continent, echo=FALSE}
#allele_freq_continent <- allele_freq_continent[,c(1:8, 13, 15)]

allele_Hs_select <- allele_Hs_continent[allele_Hs_continent$POS == "9607",]

# using coordinates gathered earlier: 
plot(
  world_coord, 
  col = 'snow3', 
  border = "snow3"
)

for (i in 1:nrow(allele_Hs_select)) {
  mapplots::add.pie(
    z = c(
      allele_Hs_select$a_Hs[i], 
      1- allele_Hs_select$a_Hs[i]
      ), 
    x = allele_Hs_select$long[i], 
    y = allele_Hs_select$lat[i], 
    radius = 7, 
    col = c(
      ggplot2::alpha("cadetblue3", 0.8), ggplot2::alpha("orchid2", 0.8)
    ), 
    labels = allele_Hs_select$location[i], 
    label.dist = 1.1, 
    cex = 0.5
  )
}
```

## Generating Fst Scores

For downstream analyses it is necessary to have measures of genetic diversity
between populations. This is first used to construct a tree, which is
subsequently used in the PGLS analysis.

```{r fst_packages, echo=FALSE}
# load in required libraries
if (!require("hierfstat")) install.packages("hierfstat")

library(hierfstat)
library(tidyr)

```

#### Reading in the data

If the above script has been run previously (and you wish to continue onward
from here), the feather files generated earlier can be used to read the data
back into R.

```{r importing_feather_AF_location, echo=FALSE}

# this imports in the file from above (hence this step may not be needed in the table is already present in the environment)

# read in vcf file 
lhf_vcf <- vcfR::read.vcfR(
  file = "~/Documents/data/lhf_d/vcf/snp_sites_filtered_vcf_maf0.005_noRef_092023.recode.vcf", 
  # this file is the same as the one used for vcf processing
  verbose = TRUE
)

```

This data can be formatted to only include information on the position, country
and allele frequencies. It is then converted to a "genid" object (another type
of S4 class object) --- this was also generated earlier when calculating allele
frequencies.

```{r create_genid_pop, echo=FALSE}

# convert the VCF to genid
lhf_genid <- vcfR::vcfR2genind(
  lhf_vcf
)

# add in country data
adegenet::pop(lhf_genid) <- lhf_meta2$country

# remove countries with small Ns ...
lhf_genid <- lhf_genid[lhf_genid@pop %in% country_N]

head(lhf_genid$pop)

```

```{r calculate_fst, echo=FALSE, include=FALSE}

# Convert from GENID object to a hierfstat object
lhf_fstat <- hierfstat::genind2hierfstat(lhf_genid)

#  Calculate Nei's genetic distances between populations...
lhf_fst <- hierfstat::pairwise.neifst(
  dat = lhf_fstat, 
  diploid = FALSE
)

head(lhf_fst)
class(lhf_fst)
```

As this takes a little while to run it is worth saving the output in a separate
file:

```{r save_pairwise_fst,, echo=FALSE}
# save the data (as this takes a while to run)
feather::write_feather(
  x = as.data.frame(lhf_fst), 
  path = "~/Documents/data/lhf_d/lhf_pairwise_fst_filtered_maf0.005_112023.feather"
)
```

If already run, the data can be imported with:

```{r load_pairwise_fst}
lhf_fst_filtered <- feather::read_feather(
   path = "~/Documents/data/lhf_d/lhf_pairwise_fst_filtered_maf0.005_112023.feather"
)
```

### Creating trees

The computed Fst scores can be used directly to create a phylogenetic tree, or
the can be used to calculate the genetic distance between populations (which is
then used for tree building purposes).

As no out-group is available, we midpoint root this tree. A key assumption of
this is that all branches are evolving at the same rate (- is this true here??)

```{r building_trees, echo=FALSE}

# generate a tree from the fst values
lhf_tree <- ape::njs(lhf_fst)  # this requires the data to be in a matrix format

# midpoint root the tree
rooted_tree <- phangorn::midpoint(lhf_tree)

# create a new tibble for the tree metadata 
tree_meta <- dplyr::as_tibble(rooted_tree$tip.label)
names(tree_meta) <- "country"

# combine populations (countries) with the associated metadata
tree_meta <- dplyr::left_join(
  tree_meta, 
  country_codes,  # this is queried above
  by = join_by("country"=="NAME")
)

# select only country, iso3, subcontinent and continent 
tree_meta <- tree_meta[,c(1,2,8,10)]
names(tree_meta)[c(2,3)] <- c("iso3", "subcontinent")
```

For aesthetic purposes, colors can be assigned to different continents:

```{r set_tree_colours, echo=FALSE}

# create a new tibble with continent list
tree_colour <- dplyr::as_tibble(
  unique(
    tree_meta$continent
  )
)

# assign a colour to each continent
tree_colour$cont_colour <- c(
  "darkorchid3", "firebrick3", "chartreuse3", 
  "chartreuse4", "turquoise", "deepskyblue4" # "lightgrey" - remove if no NAs
)

# combine this with the tree metadata table 
tree_meta <- dplyr::left_join(
  x = tree_meta, 
  y = tree_colour, 
  by = join_by("continent"=="value")
)

# create a vector with the associated colour for each country (split by continent)
tree_colours <- tree_meta$cont_colour


```

```{r midpoint_root, include=TRUE, fig.height=7, fig.width=7}

ape::plot.phylo(
  x = rooted_tree, 
  type = "fan",
  use.edge.length = FALSE, 
  cex = 0.7, 
  #srt = 20, 
  label.offset = 0.05, 
  tip.color = tree_colours
)

```

Saving the tree:

```{r saving_tree}
ape::write.tree(
  rooted_tree, 
  file = "~/Documents/data/lhf_d/lhf_pairwise_tree_rooted_midpoint.newick"
)
```

#### Filtering the tree

To ensure that the tree is as accurate as possible, countries with a small
number of individuals will be removed from the analysis:

```{r filtered-tree, fig.height=7, fig.width=7}
# looking at problem countries on the tree... 
# removing Canada, Finland, Netherlands, and Belarus (for now)
problem_countries <- c("Canada", "Belarus", "Finland", "Netherlands")

rooted_tree_filtered <- ape::drop.tip(
  phy = rooted_tree, 
  tip = problem_countries, 
  trim.internal = TRUE
)

# create a new tibble for the tree metadata 
tree_meta <- dplyr::as_tibble(rooted_tree_filtered$tip.label)
names(tree_meta) <- "country"

# combine populations (countries) with the associated metadata
tree_meta <- dplyr::left_join(
  tree_meta, 
  country_codes,  # this is queried above
  by = join_by("country"=="NAME")
)

# select only country, iso3, subcontinent and continent 
tree_meta <- tree_meta[,c(1,2,8,10)]
names(tree_meta)[c(2,3)] <- c("iso3", "subcontinent")

tree_colour <- dplyr::as_tibble(
  unique(
    tree_meta$continent
  )
)

# assign a colour to each continent
tree_colour$cont_colour <- c(
  "darkorchid3", "firebrick3", "chartreuse3", 
  "chartreuse4", "turquoise", "deepskyblue4" # "lightgrey" - remove if no NAs
)

# combine this with the tree metadata table 
tree_meta <- dplyr::left_join(
  x = tree_meta, 
  y = tree_colour, 
  by = join_by("continent"=="value")
)

# create a vector with the associated colour for each country (split by continent)
tree_colours <- tree_meta$cont_colour

ape::plot.phylo(
  x = rooted_tree_filtered, 
  type = "fan",
  use.edge.length = FALSE, 
  cex = 0.7, 
  #srt = 20, 
  label.offset = 0.05, 
  tip.color = tree_colours
)

```

```{r}
# save this tree 
ape::write.tree(
  rooted_tree_filtered, 
  file = "~/Documents/data/lhf_d/lhf_pairwise_tree_rooted_midpoint_filtered.newick"
)
```

We can test the quality of this tree by performing a bootstrapping analysis ---
how does this work for a distance matrix created from population Fst values?
Surely it is the same every time?

## PGLS Analysis & Descriptive Correlations

This section focuses on on how to perform the PGLS analysis (between different
populations) -

```{r pgls libraries needed, echo=FALSE, message=FALSE, warning=FALSE}
if (!require("psych")) install.packages("psych")
if (!require("psychTools")) install.packages("psychTools")
if (!require("caper")) install.packages("caper")

library(psych)
library(psychTools)
library(tidyr)
library(caper)
library(dplyr)
library(MuMIn)
library(stringr)
library(ggplot2)
library(Hmisc)

```

Reading in the data:

This section requires information on the allele frequencies split over
populations and the tree (both generated above).

```{r pgls_required_data}

allele_freq_location <- feather::read_feather(
  path = "~/Documents/data/lhf_d/lhf_country_allele_freq_maf0.005.feather"
)

#lhf_env. <- feather::read_feather(
#   path = "~/Documents/data/lhf_d/lhf_country_metadata_maf0.005_11_2023.feather"
#)

# combine the environmental data with the population allele frequencies
#allele_freq_location <- dplyr::left_join(
#  x = allele_freq_location, 
#  y = lhf_env.[,-c(2,3)], 
#  by = join_by("location"=="country")
#)

# read in the tree 
rooted_tree_filtered <- ape::read.tree(
  file = "~/Documents/data/lhf_d/lhf_pairwise_tree_rooted_midpoint_filtered.newick"
)

```

#### Selecting a test set of SNPs:

```{r select_test_snps, include=FALSE}
# list all unique snps
#unique(allele_freq_location$POS)

# randomly sample 15 locations
test_snps <- sample(
  unique(allele_freq_location$POS), 
  size = 15
)

# show these locations
print(
  test_snps
) 

# Note: in order to be ordered this values have to be converted into numerics - this is not compatibility with downstream steps. 

```

```{r pivot_test_AF, echo=FALSE, include=FALSE}

# filter to only include test snps
lhf_test_AF <- allele_freq_location %>%
  dplyr::filter(
    POS %in% test_snps
  )
unique(lhf_test_AF$POS)

# convert to wide format
lhf_test_AF <- tidyr::pivot_wider(
  data = lhf_test_AF, 
  names_from = POS, 
  values_from = a_freq, 
  names_prefix = "pos_"
)

head(lhf_test_AF)

```

#### Formatting the full set of SNPs

Although the test set of SNPs will be used for the following exploratory
analysis, the full panel will be necessary for later downstream analysis.

```{r pivot_AF}

## Note: this requires the pgls_required_data chunk to have been run. 

lhf_AF <- tidyr::pivot_wider(
  data = allele_freq_location, 
  names_from = POS, 
  values_from = a_freq, 
  names_prefix = "pos_"
)

lhf_AF.df <- as.data.frame(
  lhf_AF
)

cdat <- caper::comparative.data(
  data = lhf_AF.df, 
  phy = rooted_tree_filtered,  #imported in above (if not already present)
  names.col = "pop", 
  na.omit = TRUE
)

```

#### Comparing environmental variables

```{r plot_env_corr, echo=TRUE}

# plot correlations: 
#  between environmental variables:
psych::pairs.panels(
  x = lhf_AF_wide[,c(2:12)], 
  lm = TRUE, 
  stars = TRUE, 
  hist.col = "orchid3"
)

```

```{r plot_env_corr_2}
# between some snps (and environmental variables)
psych::pairs.panels(
  x = lhf_AF_wide[,c(15:29)], 
  lm = TRUE, 
  stars = TRUE, 
  hist.col = "orchid3"
)
psych::pairs.panels(
  x = lhf_AF_wide[,c(sample(2:12, 2), sample(12:ncol(lhf_AF_wide), 10))], 
  lm = TRUE, 
  stars = TRUE, 
  hist.col = "orchid3"
)
```

```{r creating_test_cdat}
# convert the test AF from a tibble to a dataframe (so that the caper package works)
lhf_test_AF.df <- as.data.frame(
  lhf_test_AF
  )

cdat <- caper::comparative.data(
  data = lhf_test_AF.df, 
  phy = rooted_tree_filtered,  #imported in above (if not already present)
  names.col = "location", 
  na.omit = TRUE
)

```

### Model selection

Broadly speaking, there are two ways of testing models; create a full model then
test whether you can remove a variable without impacting the model OR start with
a simplistic model and build up.

#### Issues with Co-linearity

As many of the environmental variables are highly correlated (see
[plot_env_corr]) there are significant problems with creating complex models
(with more than 5 predictor variables).

A number of potential solutions were discussed in response to this:

-   **First generating a 'simplistic' model with latitude only**, *with
    significant SNPs followed up with downstream analysis focusing on specific
    environmental traits*. The assumption here is that as latitude correlates
    well with many variables and will therefore pick up any highly significant
    hits. An additional advantage is that this can be run for all SNPs, removing
    the requirement for SNP-specific model selection.

-   Selection of key "biologically relevant variables". This is similar to
    above, where instead of latitude only, specific variables of perceived
    interest are selected, with these simpler models run for all sites. This is
    advantageous as it includes more information on specific traits (making
    inferences easier), but is disadvantaged by the fact it may not result in
    the most efficient model being fitted for each site.

-   Sequential addition of uncorrelated environmental variables. (Using the
    following method...)

> **Method**
>
> Starting with latitude I sequentially look at the next environmental variable
> (in order of perceived biological relevance). If the correlation is above 0.75
> with any of the existing variables in the model, the variable is skipped. If
> the variable has no correlations stronger that 0.75 with any other variables,
> the variable is added and the process is repeated.
>
> Once all variables have been processed, this "nearly full" model is used for a
> top-down approach of model selection.

> 1.  Starting with latitude.
> 2.  tmp_max added
> 3.  tmp_min added
> 4.  isothermality skipped
> 5.  tmp_seasonality skipped
> 6.  tmp_yr skipped
> 7.  tmp_range_yr skipped
> 8.  precip_yr added
> 9.  precip_seasonality added
> 10. long added

The third method described here can scripted by:

```{r pgls_model selection}

# starting with pos_7909 ~lat 
# 
pgls_mod <- caper::pgls(
  formula = pos_9544 ~ lat , 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_9544 ~ lat + tmp_min, 
  data = cdat, 
  #lambda = "ML"
)

print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_9544 ~ lat + tmp_min + precip_yr, 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_9544 ~ lat + tmp_min + precip_yr + precip_seasonality , 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7609 ~ lat + tmp_min + precip_yr + precip_seasonality + tmp_max, 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)

pgls_mod <- caper::pgls(
  formula = pos_7609 ~ lat + tmp_min + precip_yr + precip_seasonality + tmp_max , 
  data = cdat, 
  #lambda = "ML"
)
print("AIC")
AIC(pgls_mod)
#anova_df <- anova(pgls_mod)
#anova_df
```

> The conclusion from this method is that model 2 (with latitude and minimum
> temperature) is the best fit for this specific SNP.

**While this final method has it's merits, option 1 (fitting all SNPs with
latitude only as a predictor) was chosen going forward given the issues of
running model selection for each of the SNPs separately.**

**Note: Although multiple SNPs are highly correlated, this is not an issue here
as in each model only a single SNP is given as the response variable (hence any
other SNPs become irrelevant).**

```{r model_testing, include=FALSE}
# change na options
options(na.action = "na.fail")

MuMIn::dredge(
  global.model = pgls_mod, 
  beta = "none", 
  evaluate = TRUE, 
  rank = "AICc"
)
# isothem, lat, max, min 
options(na.action = "na.omit")

```

### Assessing SNP linkage

Given many of the SNPs are in close proximity to each other, and there are very
low levels of recombination in the mitochondrial genome, there is likely a high
degree of correlation (i.e: linkage) between some sets of allele frequencies. In
an attempt to identify areas where there are exceptionally high levels of
correlation the following heat maps can be created. (In effect this are LD
plots)

```{r AF_heatmaps, fig.height=5, fig.width=10}

col_N <- ncol(lhf_AF) # calculate the total number of columns
n_split <- 1 # select the number of heatmaps you want 
             # e.g.: an n_split of 2 will plot the first half then the second half.
library(ggplot2)
for (i in 1:n_split) {
  
  if (i == 1){
    x <- 15 
  } else {
    x <- x.
  }
  
  if (i ==1) {
    y <-  i*((col_N/n_split))
    y <- base::round(y)
  } else {
    y <- i*(col_N/n_split)
    y <- base::round(y)
  }
  
  print(x)
  print(y)
  
  AF_cor <- Hmisc::rcorr(
    as.matrix(
    lhf_AF[,x:y]
    )
  )
  
  x. <- y 
  
  AF_cor$r[lower.tri(AF_cor$r)] <- NA

  AF_cor <- reshape2::melt(
    AF_cor$r,
    na.rm = TRUE
  )
  
p <- ggplot2::ggplot(
    data = AF_cor,
    ggplot2::aes(
      x = Var1,
      y = Var2,
      fill = value
      )
    ) +
    ggplot2::geom_tile(
      colour = "white"
    ) +
    scale_fill_gradient2(
      low = "deepskyblue4",
      high = "darkorchid3",
      mid = "snow3",
      midpoint = 0,
      #limit = c(0.5,1),
    ) +
 #  ggtitle(
 #     paste("AF_heatmap", i)
 #   ) +
   theme(
     axis.text.x = element_text(
       angle = 55,
       vjust = 0.5,
       hjust = 0.5,
       size = 5
       ), 
     axis.text.y = element_text(
       angle = 5,
       vjust = 0.5,
       hjust = 0.5, 
       size = 4
       ), 
     axis.title = element_blank(), 
     legend.position = "none"
   )
  
  print(p)
}
```

To calculate the matrix for a specific set of columns:

```{r AF_cor_specific}

AF_cor <- Hmisc::rcorr(
    as.matrix(
    lhf_AF[,2:458]
    )
)

# AF_cor$r[lower.tri(AF_cor$r)] <- NA

# AF_cor$r <- AF_cor$r[12:nrow((AF_cor$r)), 1:11]
AF_cor$r <- AF_cor$r[ 12:200, 1:11]

View(AF_cor$r)

AF_cor <- reshape2::melt(
  AF_cor$r,
  na.rm = TRUE
  )

message("AF_cor created")

ggplot2::ggplot(
    data = AF_cor,
    ggplot2::aes(
      x = Var2,
      y = Var1,
      fill = value
      )
    ) +
    geom_tile(
      colour = "white"
    ) +
    scale_fill_gradient2(
      low = "deepskyblue4",
      high = "darkorchid3",
      mid = "snow3",
      midpoint = 0,
      #limit = c(0.5,1),
      #space = "Lab",
      #name="Pearson\nCorrelation"
    ) +
   theme(
     axis.text.x = element_text(
       angle = 55,
       vjust = 0.5,
       hjust = 0.5,
       size = 5
       ), 
     axis.text.y = element_text(
       angle = 5,
       vjust = 0.5,
       hjust = 0.5, 
       size = 4
       ), 
    # axis.title = element_blank(), 
    # legend.position = "none"
   )
    

```

### Generating ANOVA tables

Once the model is selected, ANOVA tables can be generated for each SNP.

```{r generating_ANOVA_tables, warning=FALSE}

for (i in 13:ncol(lhf_AF.df)){
  
  # select position
  snp <- names(lhf_AF.df)[i]
  
  snp_pos <- stringr::str_extract(
    string = snp, 
    pattern = "\\d+"
  )
  
  # create formula
  formula <- as.formula(
    paste(
      snp, " ~ tmp_min "
    )
  )
  
  # run model
  mod <- caper::pgls(
    formula = formula, 
    data = cdat, 
    #lambda = "ML"
  )
  
  # create anova table:
  anova_df <- anova(mod)
  
  anova_df <- cbind(anova_df, snp_pos)
  
  anova_df <- anova_df %>%
    mutate(covariate = rownames(anova_df))
  
  if (i == 13){
    anova_scores <- anova_df
  } else {
    anova_scores <- rbind(
      anova_scores, 
      anova_df
    )
  }
  
  if (i == ncol(lhf_AF.df)){
    anova_scores$snp_pos <- as.numeric(anova_scores$snp_pos)
    
    return(anova_scores)
  }
  
}

anova_scores$labels <- anova_scores$snp_pos
anova_scores$labels[anova_scores$`Pr(>F)` >= 1e-40 ] <- ""
```

```{r bonferroni_corrections}
bon_threshold <- 0.05/(ncol(lhf_AF.df)-12)*1
```

```{r manhattan_plots, echo=FALSE}

ggplot2::ggplot(
  data = anova_scores, 
  mapping = ggplot2::aes(
    x = snp_pos, 
    y = log10(`Pr(>F)`), 
    colour = covariate 
    )
  ) + 
  ggplot2::geom_point(
  ) +
  ggplot2::scale_colour_manual(
  #  values = c("chartreuse4","darkorange3", "deepskyblue2",
  #             "lightgrey", "darkorchid3","orchid1", "black")
    values = c("deepskyblue4", "grey")
  ) +
  ggplot2::geom_line(
    y = -1*log10(0.05), 
    colour = "black", 
    size = 1
  ) +
  ggplot2::xlab(
    "SNP position"
  ) + 
  ggplot2::ylab(
    "Logged P-value"
  ) + 
  ggplot2::scale_x_continuous(
    breaks = seq(0,17000, 1000)
  ) +
  ggplot2::ylim(c(0, -100)
  ) +
  ggplot2::ylab(
    "Logged P-value"
  ) +
  ggplot2::theme(
    legend.position = "bottom", 
    axis.line = element_line(
      linewidth = 1, 
      colour = "black"
    ), 
    plot.background = element_rect(
      fill = "snow3"
    ), 
    panel.grid = element_line(
      colour = "snow3", 
      linetype = "dotdash"
    ),
    panel.grid.major.x = element_line(
      colour = "snow4", 
      linetype = "dashed"
    ), 
    legend.background = element_rect(
      fill = "snow3"
    )
  )


```

```{r}
ggplot2::ggplot(
  data = anova_scores, 
  mapping = ggplot2::aes(
    x = snp_pos, 
    y = log10(`Pr(>F)`), 
    colour = covariate
  )
  ) + 
  ggplot2::geom_point(
  ) +
  ggplot2::geom_text(
    mapping = ggplot2::aes(
      label = labels
    ), 
    size = 4,
    nudge_x = sample(1:10, 1), 
    nudge_y = 2, 
    na.rm = TRUE, 
    show.legend = FALSE
  ) + 
  ggplot2::scale_colour_manual(
  #  values = c("chartreuse4","darkorange3", "deepskyblue2",
  #             "lightgrey", "darkorchid3","orchid1", "black")
    values = c( "darkorange4","orchid", "darkorchid", "darkorchid4", "darkorange2", "firebrick3","deepskyblue4", "snow3", "snow4","lightgrey", "chartreuse3", "chartreuse4", "deepskyblue4", "lightgrey", "deepskyblue3")
  ) +
  ggplot2::geom_line(
    y = -1*log10(bon_threshold), 
    colour = "black", 
    size = 1
  ) +
  ggplot2::geom_segment(
    data = mt_loci_pos,
    ggplot2::aes(
      x = Starting, 
      xend = Ending, 
      y = -80, 
      yend = -80, 
      col = classification
    ),
    size = 4, 
    alpha = 0.4, 
    position = position_jitter(
      height = 3
    )
  ) +
  #"darkorchid3", "firebrick3", "chartreuse3", "chartreuse4", "turquoise", "deepskyblue4"
  ggplot2::xlab(
    "SNP position"
  ) + 
  ggplot2::ylab(
    "Logged P-value"
  ) + 
  ggplot2::scale_x_continuous(
    breaks = seq(0,17000, 1000)
  ) +
  ggplot2::ylim(c(0, -100)
  ) +
  ggplot2::ylab(
    "Logged P-value"
  ) +
  ggplot2::theme(
    legend.position = "bottom", 
    axis.line = element_line(
      linewidth = 1, 
      colour = "black"
    ), 
    plot.background = element_rect(
      fill = "snow3"
    ), 
    panel.grid = element_line(
      colour = "snow3", 
      linetype = "dotdash"
    ),
    panel.grid.major.x = element_line(
      colour = "snow4", 
      linetype = "dashed"
    ), 
    legend.background = element_rect(
      fill = "snow3"
    )
  )
```

#### Reading in Gene Positions

Taking csv from MITOMAP

```{r}
mt_loci_pos <- read.csv(
  file = "~/Documents/data/lhf_d/GenomeLoci_MITOMAP_Foswiki.csv"
)

unique(mt_loci_pos$Shorthand)

mt_gene <- c("ATPase6", "ATPase8", "COI", "COII", "COIII", "Cytb", "ND1", 
             "ND2", "ND3", "ND4", "ND4L", "ND5", "ND6", "16S", "12S")

mt_gene_pos <- mt_loci_pos[mt_loci_pos$Shorthand %in% c(mt_gene),]

unique(mt_gene_pos$Shorthand)
```

```{r }
mt_loci_pos$classification <- mt_loci_pos$Shorthand

mt_loci_pos$Starting <- as.numeric(mt_loci_pos$Starting)
mt_loci_pos$Ending <- as.numeric(mt_loci_pos$Ending)

mt_loci_pos <- mt_loci_pos %>%
  filter( Starting >= 550 ) %>%
  filter( Starting <= 16025) %>%
  filter( Shorthand != "CR:HVS1/HV1")

unique(mt_loci_pos$classification)

mt_loci_names <- tidyr::tibble(
  regex_used = c(
    "CR.*", "12S|16S", "ND.*","Cytb" ,"CO.*", "ATP.*", "NC.*", "PH2|Humanin|MOTS-C|-|OL"
  ), 
  replace_with = c(
    "C_R", "rRNA", "C_I", "C_III", "C_IV", "C_V", "NC", "other"
  )
)

for (i in 1:nrow(mt_loci_names)) {
  mt_loci_pos$classification <- stringi::stri_replace_all(
    str = mt_loci_pos$classification, 
    regex = mt_loci_names$regex_used[i], 
    replacement = mt_loci_names$replace_with[i]
  )
}

mt_loci_pos$classification[
  grepl(
    pattern = "tRNA.*",
    mt_loci_pos$Description
  )
] <- "tRNA"

# duplicate the CR and ATT rows (to control for the fact that they loop around)
mt_loci_origin <- mt_loci_pos[grepl(
        pattern = "C_R|ATT", 
        mt_loci_pos$classification
      ),]
mt_loci_origin$Starting <- 1

# replace ending with end of 'loop'
mt_loci_pos$Ending[mt_loci_pos$Ending <= 600] <- 16506

mt_loci_pos <- dplyr::bind_rows(
  mt_loci_pos, 
  mt_loci_origin
)

mt_loci_pos$length <- (mt_loci_pos$Ending-mt_loci_pos$Starting) + 1

```

```{r classification_summary}

classification_length <- mt_loci_pos %>%
  group_by(classification) %>%
  reframe(
    total_length = sum(length)
  )

```

#### SNP loci distributions

This section is looking at which significant SNPs fall in each of the loci types
(Coding, Non-coding, tRNA).

Creating the combined tibble:

```{r sig_loci_pos}

sig_loci_pos <- dplyr::left_join(
  x = anova_scores, 
  y = mt_loci_pos, 
  by = join_by(snp_pos >= Starting, snp_pos <= Ending)
)

# NOTE: ATT and CR overlap... hence some positions are counted twice



```

```{r loci_statistics}

loci_stat <- sig_loci_pos %>%
  filter(covariate == "lat") %>%
  group_by(classification) %>%
  summarise(
    N = n()
  )

loci_stat <- dplyr::left_join(
  x = loci_stat, 
  y = classification_length, 
  by = join_by(classification)
)

loci_stat <- loci_stat %>%
  dplyr::mutate(per_snp = (N/total_length)*100)
  
loci_stat$classification <- as.factor(loci_stat$classification)

loci_stat$classification <- factor(
  x = loci_stat$classification, 
  levels = c(
    "ATT", "C_I", "C_III", "C_IV", "C_V", "C_R", "NC", "other", "rRNA", "tRNA" 
  )      
)
levels(loci_stat$classification)

ggplot2::ggplot(
  data = loci_stat, 
   mapping = aes(
    x = classification, 
    y = per_snp, 
    fill = classification
  )
  ) +
  ggplot2::geom_bar(
    stat = "identity"
  ) +
  scale_fill_manual(
   values =  c( "lightgrey","orchid", "darkorchid", "darkorchid4", "deepskyblue4", "firebrick3", "snow3", "snow4", "chartreuse3", "chartreuse4")
  ) +
  theme_minimal()
  
```
